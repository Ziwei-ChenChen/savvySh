[{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"savvySh R package implements suite shrinkage estimators multivariate linear regression. motivation shrinkage estimation originates Stein’s paradox, shows possible improve classical Ordinary Least Squares (OLS) estimator estimating multiple parameters simultaneously. improvement achieved introducing structured bias reduces variance, yielding estimators smaller overall Mean Squared Error (MSE). savvySh package provides unified interface four shrinkage strategies, designed improve prediction accuracy estimation stability finite samples. methods require cross-validation tuning parameter selection, exception Shrinkage Ridge Regression (SRR), shrinkage intensity chosen minimizing explicit MSE criterion. package particularly suited high-dimensional settings cases design matrix exhibits near multicollinearity. Empirical results simulation real data settings show least one shrinkage estimators consistently performs well better OLS, many cases leads substantial improvements, especially applied generalized linear models. details, please see: Asimit, V., Cidota, M. ., Chen, Z., & Asimit, J. (2025). Slab Shrinkage Linear Regression Estimation Main features: savvySh provides four classes shrinkage estimators linear regression: Multiplicative Shrinkage: Modifies OLS estimates applying data-driven multiplicative factors. includes Stein estimator (St), Diagonal Shrinkage (DSh), general Shrinkage estimator (Sh) solves Sylvester equation. Slab Regression: Adds quadratic penalty shrink coefficients specific directions. Simple Slab Regression (SR) penalizes along fixed direction, Generalized Slab Regression (GSR) allows penalties along multiple directions, typically aligned data structure. Linear Shrinkage: Combines OLS estimator (origin) target assumes uncorrelated covariates. method designed standardized data avoids estimating intercept. Shrinkage Ridge Regression: Improves Ridge Regression (RR) shrinking covariance matrix toward diagonal matrix equal entries, balancing sample structure stable target. Inputs: primary inputs savvySh similar regression function: x: design matrix (predictor matrix) dimension n×pn\\times p. y: response vector length nn. model_class: shrinkage method use (one \"Multiplicative\", \"Slab\", \"Linear\", \"ShrinkageRR\"). Additional method-specific parameters (e.g., include Shor ), sensible defaults automatic selection provided. Output: output savvySh list containing several elements: call: original function call. model: data frame y x used analysis. optimal_lambda: penalty parameter used (applicable). model_class: shrinkage method used (e.g.,\"Multiplicative\", \"Slab\", \"Linear\", \"ShrinkageRR\"). coefficients: list estimated coefficients applicable estimator chosen model_class. Additional method-specific diagnostics (e.g., fitted values predicted MSE). vignette provides overview methods implemented savvySh demonstrates use simulated data real data. begin theoretical overview shrinkage class, walk simulation examples illustrate apply savvySh method.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"theoretical-overview","dir":"Articles","previous_headings":"","what":"Theoretical Overview","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"savvySh function encompasses four shrinkage strategies linear regression. methods aim improve predictive accuracy interpretability trading small amount bias larger reduction variance. briefly summarize theoretical background method. Consider response vector 𝐲∈ℜn\\mathbf{y} \\\\Re^n predictor matrix 𝐗∈ℜn×(p+1)\\mathbf{X} \\\\Re^{n \\times (p+1)}. Let 𝛃̂OLS\\widehat{\\boldsymbol{\\beta}}^{\\text{OLS}} OLS estimator coefficient vector, Σ=𝐗⊤𝐗\\Sigma = \\mathbf{X}^\\top \\mathbf{X} Gram matrix design.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"multiplicative-shrinkage","dir":"Articles","previous_headings":"Theoretical Overview","what":"Multiplicative Shrinkage","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"class estimators applies multiplicative adjustments OLS coefficients using matrix 𝐃\\mathbf{D}, estimator takes form 𝛃̂=𝐃𝛃̂OLS\\widehat{\\boldsymbol{\\beta}} = \\mathbf{D} \\widehat{\\boldsymbol{\\beta}}^{OLS}. matrix 𝐃\\mathbf{D} chosen minimize MSE. Three common forms : Stein Estimator (St): St estimator shrinks coefficients uniformly single factor *̂\\widehat{^*}: 𝛃̂St=*̂𝛃̂OLS,wherea*̂=(𝛃̂OLS)T𝛃̂OLS(𝛃̂OLS)T𝛃̂OLS+MSE(𝛃̂OLS)̂. \\widehat{\\boldsymbol{\\beta}}^{\\text{St}} = \\widehat{^*} \\widehat{\\boldsymbol{\\beta}}^{\\text{OLS}}, \\quad \\text{} \\quad \\widehat{^*} = \\frac{\\left(\\widehat{\\boldsymbol\\beta}^{OLS}\\right)^T\\widehat{\\boldsymbol\\beta}^{OLS}}{\\left(\\widehat{\\boldsymbol\\beta}^{OLS}\\right)^T\\widehat{\\boldsymbol\\beta}^{OLS} + \\widehat{\\text{MSE}\\left(\\widehat{\\boldsymbol{\\beta}}^{\\text{OLS}}\\right)}}. corresponds 𝐃=a𝐈p+1\\mathbf{D} = \\mathbf{}_{p+1}. reduces variance scaling coefficients equally. Diagonal Shrinkage Estimator (DSh): Extending St estimator, DSh applies separate shrinkage factor bk*̂\\widehat{b_k^*} coefficient: 𝛃̂DSh=diag(𝐛*̂),wherebk*̂=(β̂kOLS)2(β̂kOLS)2+σ2̂σk. \\widehat{\\boldsymbol{\\beta}}^{\\text{DSh}} = \\text{diag}\\left(\\widehat{\\mathbf{b^*}}\\right), \\quad  \\text{} \\quad \\widehat{b_k^*} = \\frac{\\left(\\widehat{\\beta}_k^{\\text{OLS}}\\right)^2}{\\left(\\widehat{\\beta}_k^{\\text{OLS}}\\right)^2 + \\widehat{\\sigma^2} \\sigma_k}. corresponds 𝐃=diag(𝐛)\\mathbf{D} = \\text{diag}\\left(\\mathbf{b}\\right) σk\\sigma_k kthk^{\\text{th}} diagonal entry Σ−1\\Sigma^{-1} σ2̂\\widehat{\\sigma^2} estimated residual variance. Shrinkage Estimator (Sh): Sh estimator general form multiplicative shrinkage family. applies full (non-diagonal) matrix 𝐂*̂\\widehat{\\mathbf{C}^*} OLS estimate requires solving Sylvester equation: 𝛃̂Sh=𝐂*̂𝛃̂OLS,𝐂*̂ solves Sylvester equation:Σ−1𝐂+𝐂𝛃̂OLS(𝛃̂OLS)T=𝛃̂OLS(𝛃̂OLS)T. \\widehat{\\boldsymbol{\\beta}}^{\\text{Sh}} = \\widehat{\\mathbf{C}^*} \\widehat{\\boldsymbol{\\beta}}^{\\text{OLS}}, \\quad \\text{} \\widehat{\\mathbf{C}^*} \\text{ solves Sylvester equation:} \\ \\Sigma^{-1} \\mathbf{C} + \\mathbf{C} \\widehat{\\boldsymbol{\\beta}}^{\\text{OLS}} \\left(\\widehat{\\boldsymbol{\\beta}}^{\\text{OLS}}\\right)^T = \\widehat{\\boldsymbol{\\beta}}^{\\text{OLS}} \\left(\\widehat{\\boldsymbol{\\beta}}^{\\text{OLS}}\\right)^T. corresponds 𝐃=diag(𝐂)\\mathbf{D} = \\text{diag}\\left(\\mathbf{C}\\right). Unlike St DSh, Sh estimator allows interactions across coefficients -diagonal entries 𝐂*̂\\widehat{\\mathbf{C}^*}, capturing richer shrinkage structures.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"slab-regression","dir":"Articles","previous_headings":"Theoretical Overview","what":"Slab Regression","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"Slab regression introduces penalty shrinks estimated coefficients along specific directions, special case Generalized LASSO estimator introduced Tibshirani Taylor (2011). penalty quadratic controls much shrinkage applied direction. directions can fixed (e.g., equal 𝐮=𝟏\\textbf{u} = \\mathbf{1}) determined structure data (e.g., eigenvectors Σ\\Sigma). (Simple) Slab Regression (SR): SR estimator adds penalty based single direction vector 𝐮∈ℜRp+1\\mathbf{u} \\\\Re{R}^{p+1}, often chosen constant vector 𝟏\\mathbf{1}. estimator defined : 𝛃̂SR(μ;𝐮):=argmin𝛃∈ℜp+1(∑=1n(yi−𝐱i⊤𝛃)2+μ(𝐮⊤𝛃)2), \\widehat{\\boldsymbol{\\beta}}^{SR}\\left(\\mu;\\textbf{u}\\right) := \\arg\\min_{\\boldsymbol{\\beta}\\\\Re^{p+1}} \\left( \\sum_{=1}^n \\big(y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\big)^2 + \\mu \\big(\\mathbf{u}^\\top \\boldsymbol{\\beta}\\big)^2 \\right), μ≥0\\mu \\geq 0 controls amount shrinkage along 𝐮\\mathbf{u} closed-form. Generalized Slab Regression (GSR): GSR estimator extends SR allowing shrinkage multiple directions: 𝛃̂GSR(𝛍):=argmin𝛃∈ℜp+1(∑=1n(yi−𝐱i⊤𝛃)2+∑l∈Lμl(𝐮l⊤𝛃)2), \\widehat{\\boldsymbol{\\beta}}^{GSR}(\\boldsymbol{\\mu}) := \\arg\\min_{\\boldsymbol{\\beta} \\\\Re^{p+1}}  \\left(\\sum_{=1}^n \\big(y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\big)^2 + \\sum_{l \\L} \\mu_l \\big(\\mathbf{u}_l^\\top \\boldsymbol{\\beta}\\big)^2 \\right), μl≥0\\mu_l \\geq 0 controls shrinkage along direction specified 𝐮l\\mathbf{u}_l, LL represents set directions. Typically, vectors 𝐮l∈ℜp+1\\mathbf{u}_l \\\\Re^{p+1} chosen eigenvectors Σ:=𝐗⊤𝐗\\Sigma := \\mathbf{X}^\\top \\mathbf{X}. generalization leads flexible potentially better-performing estimators.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"linear-shrinkage","dir":"Articles","previous_headings":"Theoretical Overview","what":"Linear Shrinkage","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"LSh estimator combines OLS estimator (computed origin) target estimator assumes uncorrelated covariates. target estimator given 𝛃̂ind=Σ̃−1𝐗T𝐲\\widehat{\\boldsymbol{\\beta}}^{ind} = \\widetilde{\\Sigma}^{-1}\\mathbf{X}^T\\mathbf{y}, Σ̃=diag(Σ)\\widetilde{\\Sigma} = \\mathrm{diag}(\\Sigma). method assumes data standardized intercept needed. LSh estimator defined : 𝛃̂ind(ρ):=(1−ρ)𝛃̂̂OLS+ρ𝛃̂ind=(ρΣ̃−1Σ+(1−ρ)𝐈p)𝛃̂̂OLS:=Σ(ρ)𝛃̂̂OLS, \\widehat{\\boldsymbol{\\beta}}^{ind}(\\rho):=(1-\\rho)\\widehat{\\widehat{\\boldsymbol{\\beta}\\,}}^{OLS}+\\rho \\widehat{\\boldsymbol{\\beta}}^{ind}=\\left(\\rho\\widetilde{\\Sigma}^{-1}\\Sigma + (1-\\rho)\\textbf{}_p\\right)\\widehat{\\widehat{\\boldsymbol{\\beta}\\,}}^{OLS}:=\\Sigma(\\rho)\\widehat{\\widehat{\\boldsymbol{\\beta}\\,}}^{OLS}, 𝛃̂̂OLS\\widehat{\\widehat{\\boldsymbol{\\beta}}}^{OLS} OLS estimator without intercept, ρ\\rho shrinkage intensity chosen reduce MSE.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"shrinkage-ridge-regression","dir":"Articles","previous_headings":"Theoretical Overview","what":"Shrinkage Ridge Regression","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"SRR estimator improves ridge regression shrinking sample covariance matrix Σ\\Sigma toward scaled identity matrix v𝐈p+1v \\mathbf{}_{p+1}, v=1p+1Tr(Σ)v = \\frac{1}{p+1} \\mathrm{Tr}(\\Sigma). idea follows approach Ledoit Wolf(2004), SRR selects shrinkage level ρ\\rho directly minimizing MSE regression coefficients, covariance. SRR estimator given : 𝛃̂SRR(ρ)=(Σ*(ρ))−1𝐗T𝐲withΣ*(ρ)=(1−ρ)Σ+ρv𝐈p+1. \\widehat{\\boldsymbol{\\beta}}^{SRR}(\\rho)=\\big(\\Sigma^*(\\rho)\\big)^{-1}\\textbf{X}^T\\textbf{y} \\quad \\text{} \\quad \\Sigma^*(\\rho)=(1-\\rho)\\Sigma+\\rho v \\textbf{}_{p+1}. optimal shrinkage parameter ρ*\\rho^* chosen minimize MSE(𝛃̂SRR(ρ))MSE\\big(\\widehat{\\boldsymbol{\\beta}}^{SRR}(\\rho)\\big).","code":""},{"path":[]},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"simulation-examples","dir":"Articles","previous_headings":"","what":"Simulation Examples","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"section presents set simulation studies demonstrate performance shrinkage estimators implemented savvySh. compare OLS using synthetic datasets. performance metric L2L_2 distance estimated coefficients true parameter vector. Lower L2L_2 values indicate better recovery true coefficients. explore several generative scenarios based design matrix error distribution. structure design matrix includes different forms correlation dependence. simulation followed table comparing L2L_2 distances estimated coefficients. following data-generating processes (DGPs) considered: Multivariate Gaussian distributed covariates Toeplitz covariance matrix - N(𝛍,𝚿(ρ))N(\\pmb{\\mu}, \\pmb{\\Psi}(\\rho)) 𝚿st(ρ)=ρ|s−t|\\pmb{\\Psi}_{st}(\\rho) = \\rho^{|s-t|} 1≤s,t≤p1 \\leq s, t \\leq p, , 𝛍=(μ1,μ2,…,μp)T\\pmb{\\mu} = (\\mu_1, \\mu_2, \\ldots, \\mu_p)^T mean vector, 𝚿(ρ)\\pmb{\\Psi}(\\rho) covariance matrix, ρ\\rho represents correlation coefficient controls dependence covariates. response generated using normal noise. Student’s tt distributed Multivariate Gaussian distributed covariates Toeplitz covariance matrix - N(𝛍,𝚿(ρ))N(\\pmb{\\mu}, \\pmb{\\Psi}(\\rho)) 𝚿st(ρ)=ρ|s−t|\\pmb{\\Psi}_{st}(\\rho) = \\rho^{|s-t|} 1≤s,t≤p1 \\leq s, t \\leq p. , 𝛍=(μ1,μ2,…,μp)T\\pmb{\\mu} = (\\mu_1, \\mu_2, \\ldots, \\mu_p)^T mean vector, 𝚿(ρ)\\pmb{\\Psi}(\\rho) covariance matrix, ρ\\rho represents correlation coefficient controls dependence covariates. response includes heavier-tailed variation. Multivariate Gaussian Copula Binomial marginal distributed covariates Toeplitz covariance matrix - 𝐙i∼N(𝟎,Ψ(ρ))\\textbf{Z}_i\\sim N(\\textbf{0}, \\Psi(\\rho)) Xik=F−1(Φ(Zik))X_{ik} = F^{-1}(\\Phi(Z_{ik})), 1≤k≤p1 \\leq k \\leq p Φ\\Phi cumulative distribution function (CDF) N(0,1)N(0,1), F−1F^{-1} inverse CDF binomial distribution. Latent Space Features - Covariates generated combining low-rank structure random variations, Specifically, simulate 𝐗=𝐀𝐙+𝐄\\mathbf{X} = \\mathbf{} \\mathbf{Z} + \\mathbf{E}, 𝐀\\mathbf{} n×fn \\times f matrix factor loadings entries drawn independently standard normal distribution N(0,1)N(0,1), 𝐙\\mathbf{Z} f×pf \\times p matrix latent factors, also entries drawn independently N(0,1)N(0,1). term 𝐄\\mathbf{E} n×pn \\times p matrix independent Gaussian noise variance 10−610^{-6}, .e., N(0,10−6)N(0,10^{-6}). four settings, true regression coefficient vector alternates sign increasing magnitude, making possible assess shrinkage effect across varying coefficient scales.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"multiplicative-shrinkage-and-slab-regression","dir":"Articles","previous_headings":"Simulation Examples","what":"Multiplicative Shrinkage and Slab Regression","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"section shows savvySh implements multiplicative shrinkage including St, DSh, Sh; Slab Shrinkage including SR GSR. method compared OLS.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"multivariate-gaussian-distribution","dir":"Articles","previous_headings":"Simulation Examples > Multiplicative Shrinkage and Slab Regression","what":"Multivariate Gaussian Distribution","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"example corresponds Multivariate Gaussian distributed covariates Toeplitz covariance matrix setup described . simulate design matrix multivariate normal distribution Toeplitz covariance matrix. response generated Gaussian noise variance σ2=25\\sigma^2 = 25. tables report results estimator: first table shows L2L_2 distance estimated coefficient vector ground truth, second table displays actual estimated coefficients (rounded) method alongside true values. L2 Distance Estimated True Coefficients Estimated Coefficients Method (rounded)","code":"# Load packages library(savvySh) library(MASS) library(knitr)  # Parameters set.seed(123) n_val <- 1000 p_val <- 10 rho_val <- 0.75 sigma_val <- 5 mu_val <- 0  # Correlation matrix sigma.rho <- function(rho_val, p_val) {   rho_val ^ abs(outer(1:p_val, 1:p_val, \"-\")) }  # True beta theta_func <- function(p_val) {   sgn <- rep(c(1, -1), length.out = p_val)   mag <- ceiling(seq_len(p_val) / 2)   sgn * mag }  # Simulate data Sigma <- sigma.rho(rho_val, p_val) X <- mvrnorm(n_val, mu = rep(mu_val, p_val), Sigma = Sigma) X_intercept <- cbind(1, X) beta_true <- theta_func(p_val + 1) y <- rnorm(n_val, mean = X_intercept %*% beta_true, sd = sigma_val)  # Fit models ols_fit <- lm(y ~ X) beta_ols <- coef(ols_fit)  multi_results <- savvySh(X, y, model_class = \"Multiplicative\", include_Sh = TRUE) beta_St  <- coef(multi_results, \"St\") beta_DSh <- coef(multi_results, \"DSh\") beta_Sh  <- coef(multi_results, \"Sh\")  slab_results <- savvySh(X, y, model_class = \"Slab\") beta_SR  <- coef(slab_results, \"SR\") beta_GSR <- coef(slab_results, \"GSR\") # L2 comparison l2_table <- data.frame(   Method = c(\"OLS\", \"St\", \"DSh\", \"Sh\", \"SR\", \"GSR\"),   L2_Distance = c(     sqrt(sum((beta_ols - beta_true)^2)),     sqrt(sum((beta_St  - beta_true)^2)),     sqrt(sum((beta_DSh - beta_true)^2)),     sqrt(sum((beta_Sh  - beta_true)^2)),     sqrt(sum((beta_SR  - beta_true)^2)),     sqrt(sum((beta_GSR - beta_true)^2))   ) )  kable(l2_table, digits = 4, caption = \"L2 Distance Between Estimated and True Coefficients\") # Coefficient comparison table coef_table <- data.frame(   Term = names(beta_ols),   OLS = round(beta_ols, 3),   St = round(beta_St, 3),   DSh = round(beta_DSh, 3),   Sh = round(beta_Sh, 3),   SR = round(beta_SR, 3),   GSR = round(beta_GSR, 3),   True = round(beta_true, 3) )  kable(coef_table, caption = \"Estimated Coefficients by Method (rounded)\")"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"students-t-distribution","dir":"Articles","previous_headings":"Simulation Examples > Multiplicative Shrinkage and Slab Regression","what":"Student’s t Distribution","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"example corresponds Student’s tt distributed Multivariate Gaussian distributed covariates Toeplitz covariance matrix setup. repeat design , instead Gaussian noise, add scaled tt-distributed noise degrees freedom ν=5024\\nu = \\frac{50}{24}. tables report results estimator: first table shows L2L_2 distance estimated coefficient vector ground truth, second table displays actual estimated coefficients (rounded) method alongside true values. L2 Distance Estimated True Coefficients Estimated Coefficients Method (rounded)","code":"# Load packages library(savvySh) library(MASS) library(knitr)  # Parameters set.seed(123) n_val <- 1000 p_val <- 10 rho_val <- 0.75 df_val = 50/24  mu_val <- 0  # Correlation matrix sigma.rho <- function(rho_val, p_val) {   rho_val ^ abs(outer(1:p_val, 1:p_val, \"-\")) }  # True beta theta_func <- function(p_val) {   sgn <- rep(c(1, -1), length.out = p_val)   mag <- ceiling(seq_len(p_val) / 2)   sgn * mag }  # Simulate data Sigma <- sigma.rho(rho_val, p_val) X <- mvrnorm(n_val, mu = rep(mu_val, p_val), Sigma = Sigma) X_intercept <- cbind(1, X) beta_true <- theta_func(p_val + 1) y <- as.vector(X_intercept %*% beta_true) + rt(n = n_val, df = df_val)  # Fit models ols_fit <- lm(y ~ X) beta_ols <- coef(ols_fit)  multi_results <- savvySh(X, y, model_class = \"Multiplicative\", include_Sh = TRUE) beta_St  <- coef(multi_results, \"St\") beta_DSh <- coef(multi_results, \"DSh\") beta_Sh  <- coef(multi_results, \"Sh\")  slab_results <- savvySh(X, y, model_class = \"Slab\") beta_SR  <- coef(slab_results, \"SR\") beta_GSR <- coef(slab_results, \"GSR\") # L2 comparison l2_table <- data.frame(   Method = c(\"OLS\", \"St\", \"DSh\", \"Sh\", \"SR\", \"GSR\"),   L2_Distance = c(     sqrt(sum((beta_ols - beta_true)^2)),     sqrt(sum((beta_St  - beta_true)^2)),     sqrt(sum((beta_DSh - beta_true)^2)),     sqrt(sum((beta_Sh  - beta_true)^2)),     sqrt(sum((beta_SR  - beta_true)^2)),     sqrt(sum((beta_GSR - beta_true)^2))   ) )  kable(l2_table, digits = 4, caption = \"L2 Distance Between Estimated and True Coefficients\") # Coefficient comparison table coef_table <- data.frame(   OLS = round(beta_ols, 3),   St = round(beta_St, 3),   DSh = round(beta_DSh, 3),   Sh = round(beta_Sh, 3),   SR = round(beta_SR, 3),   GSR = round(beta_GSR, 3),   True = round(beta_true, 3) )  kable(coef_table, caption = \"Estimated Coefficients by Method (rounded)\")"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"gaussian-copula-with-binomial-margins","dir":"Articles","previous_headings":"Simulation Examples > Multiplicative Shrinkage and Slab Regression","what":"Gaussian Copula with Binomial Margins","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"example corresponds Multivariate Gaussian Copula Binomial marginal distributed covariates Toeplitz covariance matrix setup. covariates transformed Gaussian copula Binomial marginals using inverse CDF transform. simulates discrete predictor variables. tables report results estimator: first table shows L2L_2 distance estimated coefficient vector ground truth, second table displays actual estimated coefficients (rounded) method alongside true values. L2 Distance Estimated True Coefficients Estimated Coefficients Method (rounded)","code":"# Load packages library(savvySh) library(MASS) library(knitr)  # Parameters set.seed(123) n_val <- 1000 p_val <- 10 rho_val <- 0.75 q_0 <- 0.01  # Correlation matrix sigma.rho <- function(rho_val, p_val) {   rho_val ^ abs(outer(1:p_val, 1:p_val, \"-\")) }  sigma.temp <- sigma.rho(rho_val, p_val)  Z <- mvrnorm(n_val, mu = rep(0, p_val), Sigma = sigma.temp)  X <- apply(Z, 2, function(z_col) qbinom(pnorm(z_col), size = 2, prob = q_0))   X_intercept <- cbind(1, X) beta_true <- c(0, runif(p_val, 0.01, 0.3)) y <- rnorm(n_val, mean = as.vector(X_intercept %*% beta_true), sd = sigma_val)  # Fit models ols_fit <- lm(y ~ X) beta_ols <- coef(ols_fit)  multi_results <- savvySh(X, y, model_class = \"Multiplicative\", include_Sh = TRUE) beta_St  <- coef(multi_results, \"St\") beta_DSh <- coef(multi_results, \"DSh\") beta_Sh  <- coef(multi_results, \"Sh\")  slab_results <- savvySh(X, y, model_class = \"Slab\") beta_SR  <- coef(slab_results, \"SR\") beta_GSR <- coef(slab_results, \"GSR\") # L2 comparison l2_table <- data.frame(   Method = c(\"OLS\", \"St\", \"DSh\", \"Sh\", \"SR\", \"GSR\"),   L2_Distance = c(     sqrt(sum((beta_ols - beta_true)^2)),     sqrt(sum((beta_St  - beta_true)^2)),     sqrt(sum((beta_DSh - beta_true)^2)),     sqrt(sum((beta_Sh  - beta_true)^2)),     sqrt(sum((beta_SR  - beta_true)^2)),     sqrt(sum((beta_GSR - beta_true)^2))   ) )  kable(l2_table, digits = 4, caption = \"L2 Distance Between Estimated and True Coefficients\") # Coefficient comparison table coef_table <- data.frame(   OLS = round(beta_ols, 3),   St = round(beta_St, 3),   DSh = round(beta_DSh, 3),   Sh = round(beta_Sh, 3),   SR = round(beta_SR, 3),   GSR = round(beta_GSR, 3),   True = round(beta_true, 3) )  kable(coef_table, caption = \"Estimated Coefficients by Method (rounded)\")"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"linear-shrinkage-1","dir":"Articles","previous_headings":"Simulation Examples","what":"Linear Shrinkage","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"section evaluates LSh, coefficient uniformly shrunk toward zero. Data mean-centered estimation.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"multivariate-gaussian-distribution-1","dir":"Articles","previous_headings":"Simulation Examples > Linear Shrinkage","what":"Multivariate Gaussian Distribution","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"example corresponds Multivariate Gaussian distributed covariates Toeplitz covariance matrix setup. simulate Gaussian design Toeplitz correlation, using additive Gaussian noise variance σ2=25\\sigma^2 = 25. Centering applied estimation. tables report results estimator: first table shows L2L_2 distance estimated coefficient vector ground truth, second table displays actual estimated coefficients (rounded) method alongside true values. L2 Distance Estimated True Coefficients Estimated Coefficients Method (rounded)","code":"# Load packages library(savvySh) library(MASS) library(knitr)  # Parameters set.seed(123) n_val <- 1000 p_val <- 10 rho_val <- 0.75 sigma_val <- 5 mu_val <- 0  # Correlation matrix sigma.rho <- function(rho_val, p_val) {   rho_val ^ abs(outer(1:p_val, 1:p_val, \"-\")) }  # True beta theta_func <- function(p_val) {   sgn <- rep(c(1, -1), length.out = p_val)   mag <- ceiling(seq_len(p_val) / 2)   sgn * mag }  # Simulate data Sigma <- sigma.rho(rho_val, p_val) X <- mvrnorm(n_val, mu = rep(mu_val, p_val), Sigma = Sigma) X_centred <- scale(X, center = TRUE, scale = FALSE) beta_true <- theta_func(p_val) y <- rnorm(n_val, mean = X_centred %*% beta_true, sd = sigma_val) y_centred <- scale(y, center = TRUE, scale = FALSE)  # Fit models ols_fit <- lm(y_centred ~ X_centred-1) beta_ols <- coef(ols_fit)  linear_results <- savvySh(X_centred, y_centred, model_class = \"Linear\") beta_LSh <- coef(linear_results, \"LSh\") # L2 comparison l2_table <- data.frame(   Method = c(\"OLS\", \"LSh\"),   L2_Distance = c(     sqrt(sum((beta_ols - beta_true)^2)),     sqrt(sum((beta_LSh  - beta_true)^2))   ) )  kable(l2_table, digits = 4, caption = \"L2 Distance Between Estimated and True Coefficients\") # Coefficient comparison table coef_table <- data.frame(   OLS = round(beta_ols, 3),   LSh = round(beta_LSh, 3),   True = round(beta_true, 3) )  kable(coef_table, caption = \"Estimated Coefficients by Method (rounded)\")"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"students-t-distribution-1","dir":"Articles","previous_headings":"Simulation Examples > Linear Shrinkage","what":"Student’s t Distribution","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"example corresponds Student’s tt distributed Multivariate Gaussian distributed covariates Toeplitz covariance matrix setup. repeat setup replace Gaussian noise tt-distributed noise degrees freedom ν=5024\\nu = \\frac{50}{24}. Centering applied estimation. tables report results estimator: first table shows L2L_2 distance estimated coefficient vector ground truth, second table displays actual estimated coefficients (rounded) method alongside true values. L2 Distance Estimated True Coefficients Estimated Coefficients Method (rounded)","code":"# Load packages library(savvySh) library(MASS) library(knitr)  # Parameters set.seed(123) n_val <- 1000 p_val <- 10 df_val = 50/24  sigma_val <- 5 mu_val <- 0  # Correlation matrix sigma.rho <- function(rho_val, p_val) {   rho_val ^ abs(outer(1:p_val, 1:p_val, \"-\")) }  # True beta theta_func <- function(p_val) {   sgn <- rep(c(1, -1), length.out = p_val)   mag <- ceiling(seq_len(p_val) / 2)   sgn * mag }  # Simulate data Sigma <- sigma.rho(rho_val, p_val) X <- mvrnorm(n_val, mu = rep(mu_val, p_val), Sigma = Sigma) X_centred <- scale(X, center = TRUE, scale = FALSE) beta_true <- theta_func(p_val) y <- as.vector(X_centred %*% beta_true) + rt(n = n_val, df = df_val) y_centred <- scale(y, center = TRUE, scale = FALSE)  # Fit models ols_fit <- lm(y_centred ~ X_centred-1) beta_ols <- coef(ols_fit)  linear_results <- savvySh(X_centred, y_centred, model_class = \"Linear\") beta_LSh <- coef(linear_results, \"LSh\") # L2 comparison l2_table <- data.frame(   Method = c(\"OLS)\", \"LSh\"),   L2_Distance = c(     sqrt(sum((beta_ols - beta_true)^2)),     sqrt(sum((beta_LSh  - beta_true)^2))   ) )  kable(l2_table, digits = 4, caption = \"L2 Distance Between Estimated and True Coefficients\") # Coefficient comparison table coef_table <- data.frame(   OLS = round(beta_ols, 3),   LSh = round(beta_LSh, 3),   True = round(beta_true, 3) )  kable(coef_table, caption = \"Estimated Coefficients by Method (rounded)\")"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"shrinkage-ridge-regression-1","dir":"Articles","previous_headings":"Simulation Examples","what":"Shrinkage Ridge Regression","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"section introduces SRR method. SRR estimator replaces standard covariance matrix used ridge regression linear shrinkage estimate blends sample covariance matrix scaled identity matrix.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"latent-space-features","dir":"Articles","previous_headings":"Simulation Examples > Shrinkage Ridge Regression","what":"Latent Space Features","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"example corresponds Latent Space Features setup. , covariates generated using latent factor model small amount additive Gaussian noise. simulates low-rank structure design matrix. compare ridge regression estimates using glmnet SRR savvySh. tables report results estimator: first table shows L2L_2 distance estimated coefficient vector ground truth, second table displays actual estimated coefficients (rounded) method alongside true values. L2 Distance Estimated True Coefficients Estimated Coefficients Method (rounded)","code":"# Load packages library(savvySh) library(MASS) library(glmnet) #> Loading required package: Matrix #> Loaded glmnet 4.1-8 library(knitr)  # Parameters set.seed(123) n_val <- 1000 p_val <- 10 f_val <- 5 sigma_val <- 5  # True beta theta_func <- function(p_val) {   sgn <- rep(c(1, -1), length.out = p_val)   mag <- ceiling(seq_len(p_val) / 2)   sgn * mag }  A <- matrix(rnorm(n_val * f_val), nrow = n_val)   Z <- matrix(rnorm(f_val * p_val), nrow = f_val)  X <- A %*% Z  # n x p matrix noise <- matrix(rnorm(n_val * p_val, sd = sqrt(10^(-6))), nrow = n_val) X_noisy <- X + noise X_intercept <- cbind(rep(1, n_val), X_noisy) beta_true <- theta_func(p_val + 1) y <- rnorm(n_val,mean=as.vector(X_intercept%*%beta_true),sd=sigma_val)  # Fit models glmnet_fit <- cv.glmnet(X, y, alpha = 0) lambda_min_RR_glmnet <- glmnet_fit$lambda.min beta_RR <- as.vector(coef(glmnet_fit, s = \"lambda.min\"))  SRR_results <- savvySh(X, y, model_class = \"ShrinkageRR\") beta_SRR <- coef(SRR_results, \"SRR\") # L2 comparison l2_table <- data.frame(   Method = c(\"RR\", \"SRR\"),   L2_Distance = c(     sqrt(sum((beta_RR - beta_true)^2)),     sqrt(sum((beta_SRR  - beta_true)^2))   ) )  kable(l2_table, digits = 4, caption = \"L2 Distance Between Estimated and True Coefficients\") # Coefficient comparison table coef_table <- data.frame(   OLS = round(beta_RR, 3),   LSh = round(beta_SRR, 3),   True = round(beta_true, 3) )  kable(coef_table, caption = \"Estimated Coefficients by Method (rounded)\")"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"real-data-analysis","dir":"Articles","previous_headings":"","what":"Real Data Analysis","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"section provides example code applying shrinkage estimators implemented savvySh real-world datasets. examples designed illustrate shrinkage methods can used applied settings generalized linear models portfolio selection. run code present specific results ; instead, provide clean, reproducible examples users adapt explore . detailed results preprocessing procedures can found main paper. cybersickness dataset used illustrating shrinkage GLMs using Poisson model. main Shrinkage GLMs function part related package ShrinkageGLMs, can find . returns_441 dataset contains portfolio returns used demonstrating shrinkage estimation financial asset allocation.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"cybersickness-data","dir":"Articles","previous_headings":"Real Data Analysis","what":"Cybersickness data","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"example demonstrates apply Poisson GLMs various shrinkage estimators dataset related cybersickness severity scores. use cybersickness_10lags dataset, included package data/cybersickness_10lags.rda. response variable ordinal grouped four categories. goal predict ordinal outcome using lagged predictors. show preprocess, fit models, evaluate prediction performance using metrics MSE, RMSE, MAE.","code":"library(savvySh) library(ShrinkageGLMs) library(MASS)  standardize_features<-function(dataset){     dataset[2:(ncol(dataset)-1)] <- as.data.frame(scale(dataset[2:(ncol(dataset)-1)]))    return(dataset) }  set_classes<-function(data){     data[,ncol(data)]<-replace(data[,ncol(data)], data[,ncol(data)]<1, 0)    data[,ncol(data)]<-replace(data[,ncol(data)], data[,ncol(data)] %in% c(1,2,2.5,3,3.5), 1)    data[,ncol(data)]<-replace(data[,ncol(data)], data[,ncol(data)] %in% c(4,5,6), 2)    data[,ncol(data)]<-replace(data[,ncol(data)], data[,ncol(data)]>=7, 3)    return(data) } fit_and_return_coefficients <- function(x, y) {      control_list <- list(maxit = 200, epsilon = 1e-6, trace = TRUE)   family_type <- poisson(link = \"log\")      # Fitting models   opt_glm2_OLS <- glm.fit2(x, y, control = control_list, family = family_type)   opt_glm2_SR <- glm.fit2_Shirnk(x, y, model_class = \"SR\", control = control_list, family = family_type)   opt_glm2_GSR <- glm.fit2_Shirnk(x, y, model_class = \"GSR\", control = control_list, family = family_type)   opt_glm2_St <- glm.fit2_Shirnk(x, y, model_class = \"St\", control = control_list, family = family_type)   opt_glm2_DSh <- glm.fit2_Shirnk(x, y, model_class = \"DSh\", control = control_list, family = family_type)   opt_glm2_Sh <- glm.fit2_Shirnk(x, y, model_class = \"Sh\", control = control_list, family = family_type)      # Return a list of coefficients   return(list(     glm2_OLS_result = opt_glm2_OLS$coefficients,     glm2_SR_result = opt_glm2_SR$coefficients,     glm2_GSR_result = opt_glm2_GSR$coefficients,     glm2_St_result = opt_glm2_St$coefficients,     glm2_DSh_result = opt_glm2_DSh$coefficients,     glm2_Sh_result = opt_glm2_Sh$coefficients   )) }  test_model <- function(glm_coefficients, data_X, data_Y) {      upper_limit <- 3  # =3 for 4 classes; =9 for 10 classes      ### Model 1 ---> OLS ###   predicted_glm2_OLS <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_OLS_result)))   predicted_glm2_OLS <- ifelse(predicted_glm2_OLS <= upper_limit, predicted_glm2_OLS, upper_limit)      ### Model 2 ---> SR ###   predicted_glm2_SR <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_SR_result)))   predicted_glm2_SR <- ifelse(predicted_glm2_SR <= upper_limit, predicted_glm2_SR, upper_limit)      ### Model 3 ---> GSR ###   predicted_glm2_GSR <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_GSR_result)))   predicted_glm2_GSR <- ifelse(predicted_glm2_GSR <= upper_limit, predicted_glm2_GSR, upper_limit)      ### Model 4 ---> St ###   predicted_glm2_St <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_St_result)))   predicted_glm2_St <- ifelse(predicted_glm2_St <= upper_limit, predicted_glm2_St, upper_limit)      ### Model 5 ---> DSh ###   predicted_glm2_DSh <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_DSh_result)))   predicted_glm2_DSh <- ifelse(predicted_glm2_DSh <= upper_limit, predicted_glm2_DSh, upper_limit)      ### Model 6 ---> Sh ###   predicted_glm2_Sh <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_Sh_result)))   predicted_glm2_Sh <- ifelse(predicted_glm2_Sh <= upper_limit, predicted_glm2_Sh, upper_limit)      print(max(predicted_glm2_OLS))   print(max(predicted_glm2_SR))      r_OLS <- c(mse(data_Y, predicted_glm2_OLS), rmse(data_Y, predicted_glm2_OLS), mae(data_Y, predicted_glm2_OLS))   r_SR <- c(mse(data_Y, predicted_glm2_SR), rmse(data_Y, predicted_glm2_SR), mae(data_Y, predicted_glm2_SR))   r_GSR <- c(mse(data_Y, predicted_glm2_GSR), rmse(data_Y, predicted_glm2_GSR), mae(data_Y, predicted_glm2_GSR))   r_St <- c(mse(data_Y, predicted_glm2_St), rmse(data_Y, predicted_glm2_St), mae(data_Y, predicted_glm2_St))   r_DSh <- c(mse(data_Y, predicted_glm2_DSh), rmse(data_Y, predicted_glm2_DSh), mae(data_Y, predicted_glm2_DSh))   r_Sh <- c(mse(data_Y, predicted_glm2_Sh), rmse(data_Y, predicted_glm2_Sh), mae(data_Y, predicted_glm2_Sh))      return(list(     results_OLS = r_OLS,     results_SR = r_SR,     results_GSR = r_GSR,     results_St = r_St,     results_DSh = r_DSh,     results_Sh = r_Sh   )) } out_of_sample_performance <- function(percentage = 0.3, no_trials = 50, datain = cybersickness_10lags, fileout = \"results10.csv\") {      data <- datain   agregated_results<-rep(0,4*3)   data<-set_classes(data)      data<-standardize_features(data)      for (r in 1:no_trials){     bound <- floor(nrow(data)*(1-percentage))      data <- data[sample(nrow(data)), ]           train <- data[1:bound, ]                 test <- data[(bound+1):nrow(data), ]          X.tilde <- train[,-ncol(train)]     X <-  X.tilde[,-1]     train_X <- as.matrix(train[,-ncol(train)])     train_Y <-  train[,ncol(train)]     glm_coefficients<-fit_and_return_coefficients(train_X, train_Y)          test_X <- as.matrix(test[,-ncol(test)])     test_Y <-  as.matrix(test[,ncol(test)])     results<-test_model(glm_coefficients, test_X, test_Y)     agregated_results<-agregated_results+unlist(results)   }      df <- data.frame(matrix(unlist(agregated_results), nrow=length(results), byrow=TRUE))   colnames(df) <- c(\"MSE\", \"RMSE\", \"MAE\")   rownames(df) <- c(\"GLM2\", \"SR\", \"GSR\", \"St\", \"DSh\", \"Sh\")   write.csv(df, fileout) }  datain <- cybersickness_10lags output_file_path <- \"results10.csv\"  run_performance_test <- out_of_sample_performance(   percentage = 0.3,            no_trials = 50,               datain = datain,     fileout = output_file_path  )"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/articles/savvySh.html","id":"portfolio-investment","dir":"Articles","previous_headings":"Real Data Analysis","what":"Portfolio Investment","title":"savvySh: Shrinkage Methods for Linear Regression Estimation","text":"second example applies shrinkage methods asset return data constructing optimal portfolios. use returns_441 dataset, included package data/returns_441.rda. dataset consists daily returns 441 stocks including index, Date formatted YYYYMMDD. rolling window, : Fit multiple regression-based models (OLS, RR, St, DSh, Sh, SR, GSR, SRR). Extract estimated coefficients compute portfolio weights. Simulate --sample portfolio performance. Compute rolling statistics: expected annual returns, volatility, Sharpe ratios.","code":"library(savvySh) library(MASS) library(glmnet) library(PerformanceAnalytics) library(lubridate) library(quadprog) library(xts)  data <- returns_441 data$Date <- as.Date(as.character(data$Date), format = \"%Y%m%d\") colnames(data)[2:442] <- paste0(\"Company\", 1:441)  training_size <- 5 * 252   testing_size  <- 3 * 21   step_size     <- 3 * 21    n_total <- nrow(data) max_windows <- floor((n_total - training_size - testing_size) / step_size) + 1 cat(\"Total rows:\", n_total, \"\\n\") cat(\"Max windows:\", max_windows, \"\\n\")  get_full_weights <- function(est_vector) {   w <- est_vector[-1]         w_last <- 1 - sum(w)       return(c(w, w_last)) } rolling_annual_expected_returns <- data.frame() rolling_annual_sharpe_ratios <- data.frame() rolling_annual_volatilities <- data.frame() for (window_index in seq_len(max_windows)) {   start_index <- 1 + (window_index - 1) * step_size   train_start <- start_index   train_end   <- start_index + training_size - 1   test_start  <- train_end + 1   test_end    <- train_end + testing_size      train_data <- data[train_start:train_end, ]   test_data  <- data[test_start:test_end, ]   train_returns <- as.matrix(train_data[, -1])     test_returns  <- as.matrix(test_data[, -1])    Y_train <- train_returns[, 441]   X_train <- matrix(Y_train, nrow = nrow(train_returns), ncol = 440) - train_returns[, 1:440]      beta_ols <- coef(lm(Y_train ~ X_train))   glmnet_fit <- cv.glmnet(X_train, Y_train, alpha = 0)   lambda_min_RR_glmnet <- glmnet_fit$lambda.min   beta_RR <- as.vector(coef(glmnet_fit, s = \"lambda.min\"))   multi_results <- savvySh(X_train, Y_train, model_class = \"Multiplicative\", include_Sh = TRUE)   slab_results <- savvySh(X_train, Y_train, model_class = \"Slab\")   srr_results <- savvySh(X_train, Y_train, model_class = \"ShrinkageRR\")      est_results <- list(     OLS  = beta_ols,     RR   = beta_RR,     St   = coef(multi_results, \"St\"),     DSh  = coef(multi_results, \"DSh\"),     Sh   = coef(multi_results, \"Sh\"),      SR   = coef(slab_results, \"SR\"),     GSR  = coef(slab_results, \"GSR\"),     SRR  = coef(srr_results, \"SRR\")   )   weights_list <- lapply(est_results, get_full_weights)   names(weights_list) <- names(est_results)      test_dates <- as.Date(test_data$Date)   test_returns_xts <- xts(test_returns, order.by = test_dates)   daily_returns_list <- lapply(weights_list, function(w) {     rp <- Return.portfolio(R = test_returns_xts, weights = w)     return(as.numeric(rp))   })      daily_values_list <- lapply(daily_returns_list, function(r) {     cum_val <- cumprod(1 + r)     return(cum_val)   })      model_names <- names(daily_returns_list)   n_test <- length(test_start:test_end)   daily_values_mat <- matrix(0, nrow = length(model_names), ncol = n_test)   daily_returns_mat <- matrix(0, nrow = length(model_names), ncol = n_test)   rownames(daily_values_mat) <- model_names   rownames(daily_returns_mat) <- model_names      for (i in seq_along(model_names)) {     daily_values_mat[i, ]  <- daily_values_list[[i]]     daily_returns_mat[i, ] <- daily_returns_list[[i]]   }      expected_daily_returns <- rowMeans(daily_returns_mat)   annual_returns <- (1 + expected_daily_returns)^252 - 1   daily_vols <- apply(daily_returns_mat, 1, sd)   annual_vols <- daily_vols * sqrt(252)   annual_sharp <- annual_returns / annual_vols    window_result_returns <- as.data.frame(t(annual_returns))   window_result_returns$Window <- window_index   rolling_annual_expected_returns <- rbind(rolling_annual_expected_returns, window_result_returns)      window_result_sharpe <- as.data.frame(t(annual_sharp))   window_result_sharpe$Window <- window_index   rolling_annual_sharpe_ratios <- rbind(rolling_annual_sharpe_ratios, window_result_sharpe)      window_result_vols <- as.data.frame(t(annual_vols))   window_result_vols$Window <- window_index   rolling_annual_volatilities <- rbind(rolling_annual_volatilities, window_result_vols)      window_label <- paste0(\"window_\", window_index)   write.csv(daily_values_mat,              file = paste0(\"daily_portfolio_values_\", window_label, \".csv\"),              row.names = TRUE)   write.csv(daily_returns_mat,              file = paste0(\"daily_portfolio_returns_\", window_label, \".csv\"),              row.names = TRUE)      cat(\"Completed window\", window_index,       \": Training rows [\", train_start, \"to\", train_end,        \"] (Dates:\", format(train_data$Date[1], \"%Y-%m-%d\"),        \"to\", format(train_data$Date[nrow(train_data)], \"%Y-%m-%d\"),        \"), Testing rows [\", test_start, \"to\", test_end,        \"] (Dates:\", format(test_data$Date[1], \"%Y-%m-%d\"),        \"to\", format(test_data$Date[nrow(test_data)], \"%Y-%m-%d\"), \")\\n\") }"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Ziwei Chen. Author, maintainer. Vali Asimit. Author. Marina Anca Cidota. Author. Jennifer Asimit. Author.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Chen Z, Asimit V, Cidota M, Asimit J (2025). savvySh: Slab Shrinkage Linear Regression Estimation. R package version 0.1.0.","code":"@Manual{,   title = {savvySh: Slab and Shrinkage Linear Regression Estimation},   author = {Ziwei Chen and Vali Asimit and Marina Anca Cidota and Jennifer Asimit},   year = {2025},   note = {R package version 0.1.0}, }"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/index.html","id":"savvysh-shrinkage-methods-for-linear-regression-estimation","dir":"","previous_headings":"","what":"Slab and Shrinkage Linear Regression Estimation","title":"Slab and Shrinkage Linear Regression Estimation","text":"savvySh package provides unified interface fitting shrinkage estimators linear regression, particularly useful presence multicollinearity high-dimensional covariates. supports four shrinkage classes: Multiplicative Shrinkage, Slab Regression, Linear Shrinkage, Shrinkage Ridge Regression. methods improve classical ordinary least squares (OLS) estimator trading small amount bias significant reduction variance. package builds theoretical work discussed : Asimit, V., Cidota, M. ., Chen, Z., & Asimit, J. (2025). Slab Shrinkage Linear Regression Estimation. Website available : https://Ziwei-ChenChen.github.io/savvySh; besides, want run real data analysis website, please follow dependency GitHub:","code":"remotes::install_github(\"Ziwei-ChenChen/ShirnkageGLMs\")"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/index.html","id":"installation-guide","dir":"","previous_headings":"","what":"Installation Guide","title":"Slab and Shrinkage Linear Regression Estimation","text":"Install development version savvySh GitHub using: installed, load package:","code":"# install.packages(\"devtools\") devtools::install_github(\"Ziwei-ChenChen/savvySh\") library(savvySh)"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Slab and Shrinkage Linear Regression Estimation","text":"savvySh provides several shrinkage estimators designed improve regression accuracy reducing MSE: Multiplicative Shrinkage: Applies shrinkage multiplying OLS estimates data-driven factors: Stein (St): Applies single global shrinkage factor coefficients. Diagonal Shrinkage (DSh): Applies separate factor coefficient. Shrinkage (Sh): Uses full matrix shrinkage operator estimated solving Sylvester equation. Slab Regression: Adds structured shrinkage based penalty terms: Slab Regression (SR): Shrinks toward fixed target direction (e.g., vector ones). Generalized Slab Regression (GSR): Shrinks toward multiple directions (e.g., eigenvectors). Linear Shrinkage (LSh): Takes weighted average OLS estimator target estimator useful standardized data. Shrinkage Ridge Regression (SRR): Modifies Ridge Regression (RR) shrinking sample covariance matrix toward multiple identity matrix. shrinkage factors computed closed form (except SRR, optimizes shrinkage intensity numerically).","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Slab and Shrinkage Linear Regression Estimation","text":"basic example shows solve common problem:","code":"# Simulated example set.seed(123) x <- matrix(rnorm(100 * 10), 100, 10) y <- rnorm(100)  # Fit shrinkage estimators fit <- savvySh(x, y, model_class = \"Multiplicative\")  # Extract coefficients coef(fit, estimator = \"St\") coef(fit, estimator = \"DSh\")"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/index.html","id":"authors","dir":"","previous_headings":"","what":"Authors","title":"Slab and Shrinkage Linear Regression Estimation","text":"Ziwei Chen – ziwei.chen.3@bayes.city.ac.uk Vali Asimit – asimit@city.ac.uk Marina Anca Cidota – cidota@fmi.unibuc.ro Jennifer Asimit – jennifer.asimit@mrc-bsu.cam.ac.uk","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Slab and Shrinkage Linear Regression Estimation","text":"package licensed MIT License.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/coef.savvySh_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Coefficients From a Slab and Shrinkage Model Object — coef.savvySh_model","title":"Extract Coefficients From a Slab and Shrinkage Model Object — coef.savvySh_model","text":"Extract estimated coefficients fitted Slab Shrinkage linear regression model object.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/coef.savvySh_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Coefficients From a Slab and Shrinkage Model Object — coef.savvySh_model","text":"","code":"# S3 method for class 'savvySh_model' coef(object, estimator = NULL, ...)"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/coef.savvySh_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Coefficients From a Slab and Shrinkage Model Object — coef.savvySh_model","text":"object fitted model object class savvySh_model. estimator specific estimator use extracting coefficients. example, \"St\", \"DSh\", etc. must match one estimators fitted savvySh_model object. Defaults first available estimator specified. ... Additional arguments passed predict method.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/coef.savvySh_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Coefficients From a Slab and Shrinkage Model Object — coef.savvySh_model","text":"named vector coefficients specified estimator.","code":""},{"path":[]},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/coef.savvySh_model.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract Coefficients From a Slab and Shrinkage Model Object — coef.savvySh_model","text":"Ziwei Chen, Vali Asimit, Marina Anca Cidota, Jennifer Asimit Maintainer: Ziwei Chen <ziwei.chen.3@bayes.city.ac.uk>","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/predict.savvySh_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict for Slab and Shrinkage Linear Regression Models — predict.savvySh_model","title":"Predict for Slab and Shrinkage Linear Regression Models — predict.savvySh_model","text":"function predicts fitted values coefficients fitted Slab Shrinkage linear regression model created using savvySh function.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/predict.savvySh_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict for Slab and Shrinkage Linear Regression Models — predict.savvySh_model","text":"","code":"# S3 method for class 'savvySh_model' predict(   object,   newx = NULL,   type = c(\"response\", \"coefficients\"),   estimator = NULL,   ... )"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/predict.savvySh_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict for Slab and Shrinkage Linear Regression Models — predict.savvySh_model","text":"object Fitted \"savvySh_model\" object returned savvySh. newx Matrix new data predictions made. Must number columns training data. argument required type = \"response\". type Type prediction required. Can \"response\" (fitted values) \"coefficients\". Defaults \"response\". estimator specific estimator use prediction. example, \"St\", \"DSh\" model_class = \"Multiplicative\", etc. must match one estimators fitted savvySh_model object. Defaults first available estimator specified. ... Additional arguments.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/predict.savvySh_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict for Slab and Shrinkage Linear Regression Models — predict.savvySh_model","text":"Predicted values type = \"response\" coefficients type = \"coefficients\".","code":""},{"path":[]},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/predict.savvySh_model.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict for Slab and Shrinkage Linear Regression Models — predict.savvySh_model","text":"Ziwei Chen, Vali Asimit, Marina Anca Cidota, Jennifer Asimit Maintainer: Ziwei Chen <ziwei.chen.3@bayes.city.ac.uk>","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/print.savvySh_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a Slab and Shrinkage Linear Regression Model Object — print.savvySh_model","title":"Print a Slab and Shrinkage Linear Regression Model Object — print.savvySh_model","text":"Print summary Slab Shrinkage linear regression model object.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/print.savvySh_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a Slab and Shrinkage Linear Regression Model Object — print.savvySh_model","text":"","code":"# S3 method for class 'savvySh_model' print(x, digits = max(3, getOption(\"digits\") - 3), ...)"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/print.savvySh_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a Slab and Shrinkage Linear Regression Model Object — print.savvySh_model","text":"x Fitted \"savvySh_model\" object. digits Significant digits printout. ... Additional print arguments.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/print.savvySh_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a Slab and Shrinkage Linear Regression Model Object — print.savvySh_model","text":"Invisibly returns data frame summarizing model (number coefficients lambda values, ).","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/print.savvySh_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Print a Slab and Shrinkage Linear Regression Model Object — print.savvySh_model","text":"function prints call produced savvySh_model object summary model, including selected model class, number non-zero coefficients estimator, applicable, optimal lambda value.","code":""},{"path":[]},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/print.savvySh_model.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print a Slab and Shrinkage Linear Regression Model Object — print.savvySh_model","text":"Ziwei Chen, Vali Asimit, Marina Anca Cidota, Jennifer Asimit Maintainer: Ziwei Chen <ziwei.chen.3@bayes.city.ac.uk>","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/savvySh.html","id":null,"dir":"Reference","previous_headings":"","what":"Slab and Shrinkage Linear Regression Estimation — savvySh","title":"Slab and Shrinkage Linear Regression Estimation — savvySh","text":"function estimates coefficients linear regression model using several shrinkage methods, including Multiplicative Shrinkage, Slab Regression, Linear shrinkage, Shrinkage Ridge Regression. method gives estimators balance bias variance applying shrinkage ordinary least squares (OLS) solution. shrinkage estimators computed based different assumptions data.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/savvySh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Slab and Shrinkage Linear Regression Estimation — savvySh","text":"","code":"savvySh(x, y, model_class = c(\"Multiplicative\", \"Slab\", \"Linear\", \"ShrinkageRR\"),                v = 1, lambda_vals = NULL, nlambda = 100, folds = 10,                foldid = FALSE, include_Sh = FALSE, exclude = NULL)"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/savvySh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Slab and Shrinkage Linear Regression Estimation — savvySh","text":"x matrix predictor variables. y vector response variable. model_class character string specifying shrinkage model use. Options \"Multiplicative\", \"Slab\", \"Linear\", \"ShrinkageRR\". default \"Multiplicative\". v numeric value controlling strength shrinkage SR estimator Slab model. Must positive number. lambda_vals vector lambda values Ridge Regression. used multicollinearity (rank deficiency) detected \"ShrinkageRR\" selected. NULL, default sequence used. nlambda number lambda values use cross-validation lambda_vals NULL. used multicollinearity present \"ShrinkageRR\" called. default 100. folds Number folds cross-validation Ridge Regression. applicable multicollinearity occurs \"ShrinkageRR\" chosen. default 10 must integer >= 3. foldid Logical. TRUE, saves fold assignments output multicollinearity detected \"ShrinkageRR\" used. default FALSE. include_Sh Logical. TRUE, includes Sh estimator \"Multiplicative\" model. exclude vector specifying columns exclude predictors.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/savvySh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Slab and Shrinkage Linear Regression Estimation — savvySh","text":"list containing following elements: call: matched function call. model: data frame y x used analysis. optimal_lambda: lambda value chosen RR x full rank. model_class: selected model class. coefficients: list estimated coefficients applicable estimator model_class. fitted_values: list fitted values estimator. pred_MSE: list prediction MSEs estimator. ridge_results (optional): list containing detailed results RR, used multicollinearity (rank deficiency)   detected x \"ShrinkageRR\" called.   element included RR applied instead OLS due rank deficiency x. contains: lambda_range: range lambda values used RR cross-validation. cvm: vector cross-validated mean squared errors lambda lambda_range. cvsd: standard deviation cross-validated mean squared errors lambda. ridge_coefficients: matrix coefficients RR lambda value,   column representing coefficients corresponding specific lambda. optimal_lambda:lambda value minimizes cross-validated mean squared error,   used final RR estimation multicollinearity detected.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/savvySh.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Slab and Shrinkage Linear Regression Estimation — savvySh","text":"Slab Shrinkage Linear Regression Estimation methodology provides set estimators balance bias variance shrinkage techniques. estimators address issues incorporating controlled shrinkage OLS solution. main function offers three classes shrinkage estimators, tailored different modeling contexts model_class. Model Classes: Multiplicative Shrinkage:     class includes three estimators use OLS coefficients starting point apply multiplicative adjustments: St - Stein estimator, shrinks coefficients toward zero reduce variance maintaining unbiasedness. DSh - Diagonal Shrinkage, coefficient shrunk individually based corresponding variance, allowing targeted shrinkage. Sh - Shrinkage estimator solves Sylvester equation optimal shrinkage; included include_Sh = TRUE. Slab Regression:  Slab Regression includes two estimators apply adaptive penalty term solution: SR - Simple Slab Regression modifies OLS objective incorporating quadratic penalty fixed direction.   penalty controlled tuning parameter denoted v. approach can viewed special case generalized lasso    require cross-validation tuning. GSR - Generalized Slab Regression extends SR allowing shrinkage along multiple directions.   case, penalty applied along eigenvectors design covariance matrix, effectively shrinking estimates   way targets selected principal components. Shrinkage Ridge Regression: Shrinkage Ridge Regression (SRR) extends standard ridge regression additionally shrinking sample covariance matrix toward spherical target (.e., diagonal matrix equal entries). extra regularization stabilizes eigenvalues yields robust coefficient estimates,  particularly predictors lie close low-dimensional subspace.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/savvySh.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Slab and Shrinkage Linear Regression Estimation — savvySh","text":"Asimit, V., Cidota, M. ., Chen, Z., & Asimit, J. (2025). Slab Shrinkage Linear Regression Estimation. (Manuscript preparation).","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/savvySh.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Slab and Shrinkage Linear Regression Estimation — savvySh","text":"Ziwei Chen, Vali Asimit, Marina Anca Cidota, Jennifer Asimit Maintainer: Ziwei Chen <ziwei.chen.3@bayes.city.ac.uk>","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/summary.savvySh_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary of a Fitted Slab and Shrinkage Linear Regression Model with Statistics — summary.savvySh_model","title":"Summary of a Fitted Slab and Shrinkage Linear Regression Model with Statistics — summary.savvySh_model","text":"function provides summary fitted savvySh_model, displaying estimated coefficients, residuals, standard errors, R-squared, F-statistics.","code":""},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/summary.savvySh_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary of a Fitted Slab and Shrinkage Linear Regression Model with Statistics — summary.savvySh_model","text":"","code":"# S3 method for class 'savvySh_model' summary(object, ...)"},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/summary.savvySh_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary of a Fitted Slab and Shrinkage Linear Regression Model with Statistics — summary.savvySh_model","text":"object fitted model object class savvySh_model. ... Additional arguments (currently unused).","code":""},{"path":[]},{"path":"https://Ziwei-ChenChen.github.io/savvySh/reference/summary.savvySh_model.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Summary of a Fitted Slab and Shrinkage Linear Regression Model with Statistics — summary.savvySh_model","text":"Ziwei Chen, Vali Asimit, Marina Anca Cidota, Jennifer Asimit Maintainer: Ziwei Chen <ziwei.chen.3@bayes.city.ac.uk>","code":""}]
