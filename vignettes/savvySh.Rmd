---
title: "savvySh: Shrinkage Methods for Linear Regression Estimation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{savvySh: Shrinkage Methods for Linear Regression Estimation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, 
  comment = "#>")
```

# Introduction

`savvySh` is an R package that implements a suite of shrinkage estimators for multivariate linear regression. The motivation for shrinkage estimation originates from Steinâ€™s paradox, which shows that it is possible to improve on the classical ordinary least squares (OLS) estimator when estimating multiple parameters simultaneously. This improvement is achieved by introducing structured bias that reduces variance, yielding estimators with smaller overall mean squared error.

The `savvySh` package provides a unified interface to four shrinkage strategies, each designed to improve prediction accuracy and estimation stability in finite samples. These methods do not require cross-validation or tuning parameter selection, with the exception of the Shrinkage Ridge Regression (SRR), for which the shrinkage intensity is chosen by minimizing an explicit mean squared error criterion.

The package is particularly suited for high-dimensional settings or cases where the design matrix exhibits near multicollinearity. Empirical results in both simulation and real data settings show that at least one of the shrinkage estimators consistently performs as well as or better than OLS, and in many cases leads to substantial improvements, especially when applied to generalized linear models.

For more details, please see:

Asimit, V., Cidota, M. A., Chen, Z., & Asimit, J. (2025). 
[Slab and Shrinkage Linear Regression Estimation](http...)

**Main features**: `savvySh` implements four classes of shrinkage estimators:

- [Multiplicative Shrinkage:](#multiplicative-shrinkage) Adjusts OLS estimates by applying theoretically computed multiplicative factors. This class includes the *Stein estimator (St)*, *Diagonal Shrinkage (DSh)*, and, optionally, the *shrinkage estimator (Sh)* that solves a Sylvester equation.
- [Slab Regression:](#slab-regression) Incorporates an adaptive quadratic penalty to selectively shrink coefficients. The *(Simple) Slab Regression (SR)* applies a penalty in a fixed direction, while the *Generalized Slab Regression (GSR)* extends this by allowing shrinkage along directions defined by the leading principal components of the data.
- [Linear Shrinkage:](#linear-shrinkage) Constructs a convex combination of the OLS estimator (through the origin) and a target estimator that assumes uncorrelated covariates, where the data are assumed to be standardized and centered.
- [Shrinkage Ridge Regression:](#shrinkage-ridge-regression) Extends *ridge regression (RR)* by shrinking the covariance matrix toward a spherical target matrix with equal diagonal entries. This results in a modified linear system for coefficient estimation.

**Inputs**: The primary inputs to `savvySh` are similar to those of a regression function:

- `x`: The design matrix (predictor matrix) of dimension $n\times p$.
- `y`: The response vector of length $n$.
- `model_class`: The shrinkage method to use (one of `"Multiplicative"`, `"Slab"`, `"Linear"`, or `"ShrinkageRR"`)
- Additional method-specific parameters (e.g., include `Sh`or not), with sensible defaults or automatic selection if not provided.

**Output**: The output of `savvySh` is a list containing several elements:

- `call`: the original function call.
- `model`: The data frame of `y` and `x` used in the analysis.
- `optimal_lambda`: the penalty parameter used (if applicable).
- `model_class`: the shrinkage method used (e.g.,`"Multiplicative"`, `"Slab"`, `"Linear"`, or `"ShrinkageRR"`).
- `coefficients`: A list of estimated coefficients for each applicable estimator for chosen `model_class`.
- Additional method-specific diagnostics (e.g., fitted values and predicted MSE).

This vignette provides an overview of the methods implemented in `savvySh` and demonstrates how to use them on [simulated data](#simulation-examples) and [real data](#real-data-analysis). We begin with a theoretical overview of each shrinkage class, then walk through simulation examples that illustrate how to apply `savvySh` for each method.

---

# Theoretical Overview

The `savvySh` function encompasses four shrinkage strategies for linear regression. All these methods aim to improve predictive accuracy and interpretability by trading a small amount of bias for a larger reduction in variance. Below we briefly summarize the theoretical background of each method. 

Consider a response vector $\mathbf{y} \in \Re^n$ and a predictor matrix $\mathbf{X} \in \Re^{n \times (p+1)}$. Let $\widehat{\boldsymbol{\beta}}^{\text{OLS}}$ be the OLS estimator of the coefficient vector, and $\Sigma = \mathbf{X}^\top \mathbf{X}$ the Gram matrix of the design.


## Multiplicative Shrinkage

This family of estimators applies multiplicative shrinkage to the OLS coefficients, where the shrinkage factors are computed directly from the data using closed-form expressions. These factors depend on the magnitude of the OLS estimates and the estimated variance, and aim to reduce mean squared error (MSE) by adjusting each coefficient's influence accordingly.

1. **Stein Estimator (St):**
A single scalar shrinkage factor $\widehat{a^*}$ is applied uniformly to all coefficients. It is derived to minimize MSE of the scaled estimator:
$$
\widehat{\boldsymbol{\beta}}^{\text{St}} = \widehat{a^*} \widehat{\boldsymbol{\beta}}^{\text{OLS}}, \quad \text{where} \quad
\widehat{a^*} = \frac{\left(\widehat{\boldsymbol\beta}^{OLS}\right)^T\widehat{\boldsymbol\beta}^{OLS}}{\left(\widehat{\boldsymbol\beta}^{OLS}\right)^T\widehat{\boldsymbol\beta}^{OLS} + \widehat{\text{MSE}\left(\widehat{\boldsymbol{\beta}}^{\text{OLS}}\right)}}.
$$

2. **Diagonal Shrinkage Estimator (DSh):**
Extending the St estimator, DSh applies a separate shrinkage factor $\widehat{b_k^*}$ to each coefficient:
$$
\widehat{\boldsymbol{\beta}}^{\text{DSh}} = \text{diag}\left(\widehat{b_1^*}\right), \quad 
\text{where} \quad
\widehat{b_k^*} = \frac{\left(\widehat{\beta}_k^{\text{OLS}}\right)^2}{\left(\widehat{\beta}_k^{\text{OLS}}\right)^2 + \widehat{\sigma^2} \sigma_k}.
$$
where $\sigma_k$ is the $k^{\text{th}}$ diagonal entry of $\Sigma^{-1}$ and $\widehat{\sigma^2}$ is the estimated residual variance.


3. **Shrinkage Estimator (Sh):**
The most general form, this estimator uses a (non-diagonal) matrix shrinkage operator $\widehat{\mathbf{C}^*}$ derived from minimizing the MSE. The optimal matrix solves the Sylvester equation:
$$
\widehat{\boldsymbol{\beta}}^{\text{Sh}} = \widehat{\mathbf{C}^*} \widehat{\boldsymbol{\beta}}^{\text{OLS}}, \quad \text{where } \widehat{\mathbf{C}^*} \text{ solves the Sylvester equation:} \ \Sigma^{-1} \mathbf{C} + \mathbf{C} \widehat{\boldsymbol{\beta}}^{\text{OLS}} \left(\widehat{\boldsymbol{\beta}}^{\text{OLS}}\right)^T = \widehat{\boldsymbol{\beta}}^{\text{OLS}} \left(\widehat{\boldsymbol{\beta}}^{\text{OLS}}\right)^T.
$$


## Slab Regression

Slab regression methods introduce a structured quadratic penalty that shrinks the OLS coefficients in specific directions. These directions can be fixed (e.g., equal $\textbf{u} = \mathbf{1}$) or determined by the structure of the data (e.g., eigenvectors of the $\Sigma$).

1. **(Simple) Slab Regression (SR):**
The SR estimator penalizes projection of the coefficient vector onto a fixed direction $\mathbf{u}$, often the constant vector $\textbf{u} = \mathbf{1}$:
$$
\widehat{\boldsymbol{\beta}}^{SR}\left(\mu;\textbf{u}\right) := \arg\min_{\boldsymbol{\beta}\in\Re^{p+1}} \left( \sum_{i=1}^n \big(y_i - \mathbf{x}_i^\top \boldsymbol{\beta}\big)^2 + \mu \big(\mathbf{u}^\top \boldsymbol{\beta}\big)^2 \right)
$$
The parameter $\mu \geq 0$ controls the strength of the shrinkage along $\mathbf{u}$. 

2. **Generalized Slab Regression (GSR):**
The GSR estimator extends SR by allowing shrinkage in multiple directions:
$$
\widehat{\boldsymbol{\beta}}^{GSR}(\boldsymbol{\mu}) := \arg\min_{\boldsymbol{\beta} \in \Re^{p+1}} 
\left(\sum_{i=1}^n \big(y_i - \mathbf{x}_i^\top \boldsymbol{\beta}\big)^2 + \sum_{l \in L} \mu_l \big(\mathbf{u}_l^\top \boldsymbol{\beta}\big)^2 \right),
$$

where $\mu_l \geq 0$ controls the shrinkage along the direction specified by $\mathbf{u}_l$, and $L$ represents the set of directions. Typically, the vectors $\mathbf{u}_l \in \Re^{p+1}$ are chosen as eigenvectors of $\Sigma := \mathbf{X}^\top \mathbf{X}$.


## Linear Shrinkage

The optimal linear shrinkage estimator (LSh) is a weighted average between an OLS estimator and a target estimator $\widehat{\boldsymbol{\beta}}^{ind} = \widetilde{\Sigma}^{-1}\textbf{X}^T\textbf{y}$ with $\widetilde{\Sigma}=\text{diag}(\Sigma)$ by assuming that data are standardized ($\bar{\textbf{x}}_i = 0$ and $\bar{y}=0$), which eliminates the need to estimate the intercept.

The linear shrinkage estimator is given by:
$$
\widehat{\boldsymbol{\beta}}^{ind}(\rho):=(1-\rho)\widehat{\widehat{\boldsymbol{\beta}\,}}^{OLS}+\rho \widehat{\boldsymbol{\beta}}^{ind}=\left(\rho\widetilde{\Sigma}^{-1}\Sigma + (1-\rho)\textbf{I}_p\right)\widehat{\widehat{\boldsymbol{\beta}\,}}^{OLS}:=\Sigma(\rho)\widehat{\widehat{\boldsymbol{\beta}\,}}^{OLS}, 
$$
where $\widehat{\widehat{\boldsymbol{\beta}\,}}^{OLS}$ is the OLS estimator through the origin, while $\rho$ is the shrinkage intensity estimator.

## Shrinkage Ridge Regression

The shrinkage ridge regression (SRR) estimator modifies the standard RR approach by applying shrinkage to the covariance matrix itself. Rather than shrinking toward the identity matrix, SRR shrinks the sample covariance matrix $\Sigma$ toward a scalar multiple of the identity matrix, which is similar to [Ledoit and Wolf(2004)](https://www.sciencedirect.com/science/article/pii/S0047259X03000964).

$$
\widehat{\boldsymbol{\beta}}^{SRR}(\rho)=\big(\Sigma^*(\rho)\big)^{-1}\textbf{X}^T\textbf{y} \quad \text{with} \quad \Sigma^*(\rho)=(1-\rho)\Sigma+\rho v \textbf{I}_{p+1}, 
$$
and the optimal $\rho^*$ is chosen so that $MSE\big(\widehat{\boldsymbol{\beta}}^{SRR}(\rho)\big)$ is minimized. 

## Summary of Shrinkage Estimators

| Method                       | Shrinkage Direction                  | Shrinkage Form                       | Assumes Standardized Data? | Tuning or Optimizing Required? | Handles Multicollinearity? |
|-----------------------------|--------------------------------------|--------------------------------------|-----------------------------|-------------------------------|-----------------------------|
| **OLS**                     | None                                 | No shrinkage                         | No                          | No                            | No                          |
| **RR**   | Toward origin                        | $(\mathbf{X}^\top\mathbf{X} + \lambda I)^{-1} \mathbf{X}^\top \mathbf{y}$ | No                          | **Yes** (cross-validation)    | **Yes**                     |
| **Stein (St)**              | Global (scalar)                      | $\widehat{a^*} \cdot \widehat{\beta}^{\text{OLS}}$ | No                          | No                            | No                          |
| **Diagonal Shrinkage (DSh)**| Coordinate-wise (diagonal)           | $\mathbf{D} \cdot \widehat{\beta}^{\text{OLS}}$   | No                          | No                            | No                          |
| **Shrinkage (Sh)**          | Matrix shrinkage (non-diagonal)      | $\mathbf{C} \cdot \widehat{\beta}^{\text{OLS}}$   | No                          | No                            | No                          |
| **Slab Regression (SR)**    | Fixed direction (e.g., $\mathbf{1}$) | Penalized projection                 | No                          | No                            | No                          |
| **Generalized Slab (GSR)**  | Multiple directions (eigenvectors)   | Penalized projection                 | No                          | No                            | No                          |
| **Linear Shrinkage (LSh)**  | Toward diagonal target               | Convex combination                   | **Yes**                     | No                            | No                          |
| **Shrinkage Ridge (SRR)**   | Toward scaled identity               | Modified Ridge                       | No                          | **Yes** (theoretical minimization) | **Yes**               |

---


# Simulation Examples

This section presents a set of simulation studies that demonstrate the performance of shrinkage estimators implemented in `savvySh`. We compare them against OLS using synthetic datasets. The performance metric is the $L_2$ distance between the estimated coefficients and the true parameter vector. Lower $L_2$ values indicate better recovery of the true coefficients.

We explore several generative scenarios based on the design matrix and error distribution. The structure of the design matrix includes different forms of correlation and dependence. Each simulation is followed by a table comparing $L_2$ distances and estimated coefficients.

The following data-generating processes (DGPs) are considered:

- [Multivariate Gaussian distributed covariates with Toeplitz covariance matrix](#multivariate-gaussian-distribution) -
$N(\pmb{\mu}, \pmb{\Psi}(\rho))$ where $\pmb{\Psi}_{st}(\rho) = \rho^{|s-t|}$ for all $1 \leq s, t \leq p$, Here, $\pmb{\mu} = (\mu_1, \mu_2, \ldots, \mu_p)^T$ is the mean vector, $\pmb{\Psi}(\rho)$ is the covariance matrix, and $\rho$ represents the correlation coefficient that controls the dependence between covariates. The response is generated using normal noise.
- [Student's $t$ distributed Multivariate Gaussian distributed covariates with Toeplitz covariance matrix](#students-t-distribution) - $N(\pmb{\mu}, \pmb{\Psi}(\rho))$ where $\pmb{\Psi}_{st}(\rho) = \rho^{|s-t|}$ for all $1 \leq s, t \leq p$. Here, $\pmb{\mu} = (\mu_1, \mu_2, \ldots, \mu_p)^T$ is the mean vector, $\pmb{\Psi}(\rho)$ is the covariance matrix, and $\rho$ represents the correlation coefficient that controls the dependence between covariates. The response includes heavier-tailed variation.
- [Multivariate Gaussian Copula with Binomial marginal distributed covariates and Toeplitz covariance matrix](#gaussian-copula-with-binomial-margins) - $\textbf{Z}_i\sim N(\textbf{0}, \Psi(\rho))$ with $X_{ik} = F^{-1}(\Phi(Z_{ik}))$, for $1 \leq k \leq p$ where $\Phi$ is the cumulative distribution function (CDF) of $N(0,1)$, and $F^{-1}$ is the inverse CDF of the binomial distribution.
- [Latent Space Features](#latent-space-features) - Covariates are generated by combining a low-rank structure with random variations, Specifically, we simulate $\mathbf{X} = \mathbf{A} \mathbf{Z} + \mathbf{E}$, where $\mathbf{A}$ is an $n \times f$ matrix of factor loadings with entries drawn independently from a standard normal distribution $N(0,1)$, and $\mathbf{Z}$ is an $f \times p$ matrix of latent factors, also with entries drawn independently from $N(0,1)$. The term $\mathbf{E}$ is an $n \times p$ matrix of independent Gaussian noise with variance $10^{-6}$, i.e., $N(0,10^{-6})$.

In all three settings, the true regression coefficient vector alternates in sign with increasing magnitude, making it possible to assess the shrinkage effect across varying coefficient scales.


## Multiplicative Shrinkage and Slab Regression

This section shows how `savvySh` implements multiplicative shrinkage including Stein Estimator (St), Diagonal Shrinkage (DSh), and Shrinkage (Sh); Slab Shrinkage including (Simple) Slab Regression (SR) and Generalized Slab Regression (GSR). Each method is compared to OLS.


### Multivariate Gaussian Distribution

This example corresponds to the [Multivariate Gaussian distributed covariates with Toeplitz covariance matrix](#multivariate-gaussian-distribution) setup described above. We simulate a design matrix from a multivariate normal distribution with a Toeplitz covariance matrix. The response is generated with Gaussian noise with variance $\sigma^2 = 25$.
```{r}
# Load packages
library(savvySh)
library(MASS)
library(knitr)

# Parameters
set.seed(123)
n_val <- 1000
p_val <- 10
rho_val <- 0.75
sigma_val <- 5
mu_val <- 0

# Correlation matrix
sigma.rho <- function(rho_val, p_val) {
  rho_val ^ abs(outer(1:p_val, 1:p_val, "-"))
}

# True beta
theta_func <- function(p_val) {
  sgn <- rep(c(1, -1), length.out = p_val)
  mag <- ceiling(seq_len(p_val) / 2)
  sgn * mag
}

# Simulate data
Sigma <- sigma.rho(rho_val, p_val)
X <- mvrnorm(n_val, mu = rep(mu_val, p_val), Sigma = Sigma)
X_intercept <- cbind(1, X)
beta_true <- theta_func(p_val + 1)
y <- rnorm(n_val, mean = X_intercept %*% beta_true, sd = sigma_val)

# Fit models
ols_fit <- lm(y ~ X)
beta_ols <- coef(ols_fit)

multi_results <- savvySh(X, y, model_class = "Multiplicative", include_Sh = TRUE)
beta_St  <- coef(multi_results, "St")
beta_DSh <- coef(multi_results, "DSh")
beta_Sh  <- coef(multi_results, "Sh")

slab_results <- savvySh(X, y, model_class = "Slab")
beta_SR  <- coef(slab_results, "SR")
beta_GSR <- coef(slab_results, "GSR")
```

The tables below report the results of each estimator: the first table shows the $L_2$ distance between the estimated coefficient vector and the ground truth, while the second table displays the actual estimated coefficients (rounded) for each method alongside the true values.
```{r}
# L2 comparison
l2_table <- data.frame(
  Method = c("Ordinary Least Squares (OLS)", "Stein (St)", "Diagonal Shrinkage (DSh)", "Shrinkage (Sh)",
             "Slab Regression (SR)", "Generalized Slab Regression (GSR)"),
  L2_Distance = c(
    sqrt(sum((beta_ols - beta_true)^2)),
    sqrt(sum((beta_St  - beta_true)^2)),
    sqrt(sum((beta_DSh - beta_true)^2)),
    sqrt(sum((beta_Sh  - beta_true)^2)),
    sqrt(sum((beta_SR  - beta_true)^2)),
    sqrt(sum((beta_GSR - beta_true)^2))
  )
)

kable(l2_table, digits = 4, caption = "L2 Distance Between Estimated and True Coefficients")

# Coefficient comparison table
coef_table <- data.frame(
  Term = names(beta_ols),
  OLS = round(beta_ols, 3),
  St = round(beta_St, 3),
  DSh = round(beta_DSh, 3),
  Sh = round(beta_Sh, 3),
  SR = round(beta_SR, 3),
  GSR = round(beta_GSR, 3),
  True = round(beta_true, 3)
)

kable(coef_table, caption = "Estimated Coefficients by Method (rounded)")
```

### Student's t Distribution

This example corresponds to the [Student's $t$ distributed Multivariate Gaussian distributed covariates with Toeplitz covariance matrix](#students-t-distribution) setup. We repeat the same design as above, but instead of Gaussian noise, we add a scaled $t$-distributed noise with degrees of freedom $\nu = \frac{50}{24}$.
```{r}
# Load packages
library(savvySh)
library(MASS)
library(knitr)

# Parameters
set.seed(123)
n_val <- 1000
p_val <- 10
rho_val <- 0.75
df_val = 50/24 
mu_val <- 0

# Correlation matrix
sigma.rho <- function(rho_val, p_val) {
  rho_val ^ abs(outer(1:p_val, 1:p_val, "-"))
}

# True beta
theta_func <- function(p_val) {
  sgn <- rep(c(1, -1), length.out = p_val)
  mag <- ceiling(seq_len(p_val) / 2)
  sgn * mag
}

# Simulate data
Sigma <- sigma.rho(rho_val, p_val)
X <- mvrnorm(n_val, mu = rep(mu_val, p_val), Sigma = Sigma)
X_intercept <- cbind(1, X)
beta_true <- theta_func(p_val + 1)
y <- as.vector(X_intercept %*% beta_true) + rt(n = n_val, df = df_val)

# Fit models
ols_fit <- lm(y ~ X)
beta_ols <- coef(ols_fit)

multi_results <- savvySh(X, y, model_class = "Multiplicative", include_Sh = TRUE)
beta_St  <- coef(multi_results, "St")
beta_DSh <- coef(multi_results, "DSh")
beta_Sh  <- coef(multi_results, "Sh")

slab_results <- savvySh(X, y, model_class = "Slab")
beta_SR  <- coef(slab_results, "SR")
beta_GSR <- coef(slab_results, "GSR")
```

The tables below report the results of each estimator: the first table shows the $L_2$ distance between the estimated coefficient vector and the ground truth, while the second table displays the actual estimated coefficients (rounded) for each method alongside the true values.
```{r}
# L2 comparison
l2_table <- data.frame(
  Method = c("Ordinary Least Squares (OLS)", "Stein (St)", "Diagonal Shrinkage (DSh)", "Shrinkage (Sh)",
             "Slab Regression (SR)", "Generalized Slab Regression (GSR)"),
  L2_Distance = c(
    sqrt(sum((beta_ols - beta_true)^2)),
    sqrt(sum((beta_St  - beta_true)^2)),
    sqrt(sum((beta_DSh - beta_true)^2)),
    sqrt(sum((beta_Sh  - beta_true)^2)),
    sqrt(sum((beta_SR  - beta_true)^2)),
    sqrt(sum((beta_GSR - beta_true)^2))
  )
)

kable(l2_table, digits = 4, caption = "L2 Distance Between Estimated and True Coefficients")

# Coefficient comparison table
coef_table <- data.frame(
  OLS = round(beta_ols, 3),
  St = round(beta_St, 3),
  DSh = round(beta_DSh, 3),
  Sh = round(beta_Sh, 3),
  SR = round(beta_SR, 3),
  GSR = round(beta_GSR, 3),
  True = round(beta_true, 3)
)

kable(coef_table, caption = "Estimated Coefficients by Method (rounded)")
```


### Gaussian Copula with Binomial Margins

This example corresponds to the [Multivariate Gaussian Copula with Binomial marginal distributed covariates and Toeplitz covariance matrix](#gaussian-copula-with-binomial-margins) setup. The covariates are transformed from a Gaussian copula to Binomial marginals using the inverse CDF transform. This simulates discrete predictor variables.
```{r}
# Load packages
library(savvySh)
library(MASS)
library(knitr)

# Parameters
set.seed(123)
n_val <- 1000
p_val <- 10
rho_val <- 0.75
q_0 <- 0.01

# Correlation matrix
sigma.rho <- function(rho_val, p_val) {
  rho_val ^ abs(outer(1:p_val, 1:p_val, "-"))
}

sigma.temp <- sigma.rho(rho_val, p_val) 
Z <- mvrnorm(n_val, mu = rep(0, p_val), Sigma = sigma.temp) 
X <- apply(Z, 2, function(z_col) qbinom(pnorm(z_col), size = 2, prob = q_0))  
X_intercept <- cbind(1, X)
beta_true <- c(0, runif(p_val, 0.01, 0.3))
y <- rnorm(n_val, mean = as.vector(X_intercept %*% beta_true), sd = sigma_val)

# Fit models
ols_fit <- lm(y ~ X)
beta_ols <- coef(ols_fit)

multi_results <- savvySh(X, y, model_class = "Multiplicative", include_Sh = TRUE)
beta_St  <- coef(multi_results, "St")
beta_DSh <- coef(multi_results, "DSh")
beta_Sh  <- coef(multi_results, "Sh")

slab_results <- savvySh(X, y, model_class = "Slab")
beta_SR  <- coef(slab_results, "SR")
beta_GSR <- coef(slab_results, "GSR")
```

The tables below report the results of each estimator: the first table shows the $L_2$ distance between the estimated coefficient vector and the ground truth, while the second table displays the actual estimated coefficients (rounded) for each method alongside the true values.
```{r}
# L2 comparison
l2_table <- data.frame(
  Method = c("Ordinary Least Squares (OLS)", "Stein (St)", "Diagonal Shrinkage (DSh)", "Shrinkage (Sh)",
             "Slab Regression (SR)", "Generalized Slab Regression (GSR)"),
  L2_Distance = c(
    sqrt(sum((beta_ols - beta_true)^2)),
    sqrt(sum((beta_St  - beta_true)^2)),
    sqrt(sum((beta_DSh - beta_true)^2)),
    sqrt(sum((beta_Sh  - beta_true)^2)),
    sqrt(sum((beta_SR  - beta_true)^2)),
    sqrt(sum((beta_GSR - beta_true)^2))
  )
)

kable(l2_table, digits = 4, caption = "L2 Distance Between Estimated and True Coefficients")

# Coefficient comparison table
coef_table <- data.frame(
  OLS = round(beta_ols, 3),
  St = round(beta_St, 3),
  DSh = round(beta_DSh, 3),
  Sh = round(beta_Sh, 3),
  SR = round(beta_SR, 3),
  GSR = round(beta_GSR, 3),
  True = round(beta_true, 3)
)

kable(coef_table, caption = "Estimated Coefficients by Method (rounded)")
```

## Linear Shrinkage

This section evaluates linear shrinkage estimators (LSh), where each coefficient is uniformly shrunk toward zero. Data are mean-centered before estimation.

### Multivariate Gaussian Distribution

This example corresponds to the [Multivariate Gaussian distributed covariates with Toeplitz covariance matrix](#multivariate-gaussian-distribution) setup. We simulate from a Gaussian design with Toeplitz correlation, using additive Gaussian noise with variance $\sigma^2 = 25$. **Centering is applied before estimation**.
```{r}
# Load packages
library(savvySh)
library(MASS)
library(knitr)

# Parameters
set.seed(123)
n_val <- 1000
p_val <- 10
rho_val <- 0.75
sigma_val <- 5
mu_val <- 0

# Correlation matrix
sigma.rho <- function(rho_val, p_val) {
  rho_val ^ abs(outer(1:p_val, 1:p_val, "-"))
}

# True beta
theta_func <- function(p_val) {
  sgn <- rep(c(1, -1), length.out = p_val)
  mag <- ceiling(seq_len(p_val) / 2)
  sgn * mag
}

# Simulate data
Sigma <- sigma.rho(rho_val, p_val)
X <- mvrnorm(n_val, mu = rep(mu_val, p_val), Sigma = Sigma)
X_centred <- scale(X, center = TRUE, scale = FALSE)
beta_true <- theta_func(p_val)
y <- rnorm(n_val, mean = X_centred %*% beta_true, sd = sigma_val)
y_centred <- scale(y, center = TRUE, scale = FALSE)

# Fit models
ols_fit <- lm(y_centred ~ X_centred-1)
beta_ols <- coef(ols_fit)

linear_results <- savvySh(X_centred, y_centred, model_class = "Linear")
beta_LSh <- coef(linear_results, "LSh")
```

The tables below report the results of each estimator: the first table shows the $L_2$ distance between the estimated coefficient vector and the ground truth, while the second table displays the actual estimated coefficients (rounded) for each method alongside the true values.
```{r}

# L2 comparison
l2_table <- data.frame(
  Method = c("Ordinary Least Squares (OLS)", "Linear Shrinkage (LSh)"),
  L2_Distance = c(
    sqrt(sum((beta_ols - beta_true)^2)),
    sqrt(sum((beta_LSh  - beta_true)^2))
  )
)

kable(l2_table, digits = 4, caption = "L2 Distance Between Estimated and True Coefficients")

# Coefficient comparison table
coef_table <- data.frame(
  OLS = round(beta_ols, 3),
  LSh = round(beta_LSh, 3),
  True = round(beta_true, 3)
)

kable(coef_table, caption = "Estimated Coefficients by Method (rounded)")
```

### Student's t Distribution

This example corresponds to the [Student's $t$ distributed Multivariate Gaussian distributed covariates with Toeplitz covariance matrix](#students-t-distribution) setup. We repeat the setup from above but replace Gaussian noise with $t$-distributed noise with degrees of freedom $\nu = \frac{50}{24}$. **Centering is applied before estimation**.
```{r}
# Load packages
library(savvySh)
library(MASS)
library(knitr)

# Parameters
set.seed(123)
n_val <- 1000
p_val <- 10
df_val = 50/24 
sigma_val <- 5
mu_val <- 0

# Correlation matrix
sigma.rho <- function(rho_val, p_val) {
  rho_val ^ abs(outer(1:p_val, 1:p_val, "-"))
}

# True beta
theta_func <- function(p_val) {
  sgn <- rep(c(1, -1), length.out = p_val)
  mag <- ceiling(seq_len(p_val) / 2)
  sgn * mag
}

# Simulate data
Sigma <- sigma.rho(rho_val, p_val)
X <- mvrnorm(n_val, mu = rep(mu_val, p_val), Sigma = Sigma)
X_centred <- scale(X, center = TRUE, scale = FALSE)
beta_true <- theta_func(p_val)
y <- as.vector(X_centred %*% beta_true) + rt(n = n_val, df = df_val)
y_centred <- scale(y, center = TRUE, scale = FALSE)

# Fit models
ols_fit <- lm(y_centred ~ X_centred-1)
beta_ols <- coef(ols_fit)

linear_results <- savvySh(X_centred, y_centred, model_class = "Linear")
beta_LSh <- coef(linear_results, "LSh")
```

The tables below report the results of each estimator: the first table shows the $L_2$ distance between the estimated coefficient vector and the ground truth, while the second table displays the actual estimated coefficients (rounded) for each method alongside the true values.
```{r}
# L2 comparison
l2_table <- data.frame(
  Method = c("Ordinary Least Squares (OLS)", "Linear Shrinkage (LSh)"),
  L2_Distance = c(
    sqrt(sum((beta_ols - beta_true)^2)),
    sqrt(sum((beta_LSh  - beta_true)^2))
  )
)

kable(l2_table, digits = 4, caption = "L2 Distance Between Estimated and True Coefficients")

# Coefficient comparison table
coef_table <- data.frame(
  OLS = round(beta_ols, 3),
  LSh = round(beta_LSh, 3),
  True = round(beta_true, 3)
)

kable(coef_table, caption = "Estimated Coefficients by Method (rounded)")
```


## Shrinkage Ridge Regression

This section introduces the Shrinkage Ridge Regression (SRR) method. The SRR estimator replaces the standard covariance matrix used in ridge regression with a linear shrinkage estimate that blends the sample covariance matrix with a scaled identity matrix.

### Latent Space Features

This example corresponds to the [Latent Space Features](#latent-space-features) setup. Here, covariates are generated using a latent factor model with a small amount of additive Gaussian noise. This simulates a low-rank structure in the design matrix. We compare ridge regression estimates using `glmnet` against the shrinkage ridge regression (SRR) from `savvySh`.
```{r}
# Load packages
library(savvySh)
library(MASS)
library(glmnet)
library(knitr)

# Parameters
set.seed(123)
n_val <- 1000
p_val <- 10
f_val <- 5
sigma_val <- 5

# True beta
theta_func <- function(p_val) {
  sgn <- rep(c(1, -1), length.out = p_val)
  mag <- ceiling(seq_len(p_val) / 2)
  sgn * mag
}

A <- matrix(rnorm(n_val * f_val), nrow = n_val)  
Z <- matrix(rnorm(f_val * p_val), nrow = f_val) 
X <- A %*% Z  # n x p matrix
noise <- matrix(rnorm(n_val * p_val, sd = sqrt(10^(-6))), nrow = n_val)
X_noisy <- X + noise
X_intercept <- cbind(rep(1, n_val), X_noisy)
beta_true <- theta_func(p_val + 1)
y <- rnorm(n_val,mean=as.vector(X_intercept%*%beta_true),sd=sigma_val)

# Fit models
glmnet_fit <- cv.glmnet(X, y, alpha = 0)
lambda_min_RR_glmnet <- glmnet_fit$lambda.min
beta_RR <- as.vector(coef(glmnet_fit, s = "lambda.min"))

SRR_results <- savvySh(X, y, model_class = "ShrinkageRR")
beta_SRR <- coef(SRR_results, "SRR")
```

The tables below report the results of each estimator: the first table shows the $L_2$ distance between the estimated coefficient vector and the ground truth, while the second table displays the actual estimated coefficients (rounded) for each method alongside the true values.
```{r}
# L2 comparison
l2_table <- data.frame(
  Method = c("Ridge Regression (RR)", "Shrinkage Ridge Regression (SRR)"),
  L2_Distance = c(
    sqrt(sum((beta_RR - beta_true)^2)),
    sqrt(sum((beta_SRR  - beta_true)^2))
  )
)

kable(l2_table, digits = 4, caption = "L2 Distance Between Estimated and True Coefficients")

# Coefficient comparison table
coef_table <- data.frame(
  OLS = round(beta_RR, 3),
  LSh = round(beta_SRR, 3),
  True = round(beta_true, 3)
)

kable(coef_table, caption = "Estimated Coefficients by Method (rounded)")
```

---

# Real Data Analysis

This section provides example code for applying the shrinkage estimators implemented in `savvySh` to real-world datasets. These examples are designed to illustrate how the shrinkage methods can be used in applied settings such as generalized linear models and portfolio selection. We do not run the code or present specific results here; instead, we provide clean, reproducible examples for users to adapt and explore on their own. The detailed results can be find in the [main paper](#main-paper).

- The **cybersickness dataset** is used for illustrating shrinkage GLMs using a Poisson model. The main Shirnkage GLMs function is part of the related package `ShrinkageGLMs`, you can find [here](https://github.com/Ziwei-ChenChen/GLMs).
- The **returns_441 dataset** contains portfolio returns and is used for demonstrating shrinkage estimation in financial asset allocation.

## Cybersickness data

This example demonstrates how to apply Poisson GLMs and various shrinkage estimators to a dataset related to cybersickness severity scores. We use the `cybersickness_10lags` dataset, included in the package under `data/cybersickness_10lags.rda`. The response variable is ordinal and grouped into four categories. The goal is to predict this ordinal outcome using lagged predictors. Here we show how to preprocess, fit models, and evaluate prediction performance using metrics such as MSE, RMSE, and MAE.
```{r, eval=FALSE}
library(savvySh)
library(ShrinkageGLMs)
library(MASS)

standardize_features<-function(dataset){  
  dataset[2:(ncol(dataset)-1)] <- as.data.frame(scale(dataset[2:(ncol(dataset)-1)])) 
  return(dataset)
}

set_classes<-function(data){  
  data[,ncol(data)]<-replace(data[,ncol(data)], data[,ncol(data)]<1, 0) 
  data[,ncol(data)]<-replace(data[,ncol(data)], data[,ncol(data)] %in% c(1,2,2.5,3,3.5), 1) 
  data[,ncol(data)]<-replace(data[,ncol(data)], data[,ncol(data)] %in% c(4,5,6), 2) 
  data[,ncol(data)]<-replace(data[,ncol(data)], data[,ncol(data)]>=7, 3) 
  return(data)
}
```


```{r, eval=FALSE}
fit_and_return_coefficients <- function(x, y) {
  
  control_list <- list(maxit = 200, epsilon = 1e-6, trace = TRUE)
  family_type <- poisson(link = "log")
  
  # Fitting models
  opt_glm2_OLS <- glm.fit2(x, y, control = control_list, family = family_type)
  opt_glm2_SR <- glm.fit2_Shirnk(x, y, model_class = "SR", control = control_list, family = family_type)
  opt_glm2_GSR <- glm.fit2_Shirnk(x, y, model_class = "GSR", control = control_list, family = family_type)
  opt_glm2_St <- glm.fit2_Shirnk(x, y, model_class = "St", control = control_list, family = family_type)
  opt_glm2_DSh <- glm.fit2_Shirnk(x, y, model_class = "DSh", control = control_list, family = family_type)
  opt_glm2_Sh <- glm.fit2_Shirnk(x, y, model_class = "Sh", control = control_list, family = family_type)
  
  # Return a list of coefficients
  return(list(
    glm2_OLS_result = opt_glm2_OLS$coefficients,
    glm2_SR_result = opt_glm2_SR$coefficients,
    glm2_GSR_result = opt_glm2_GSR$coefficients,
    glm2_St_result = opt_glm2_St$coefficients,
    glm2_DSh_result = opt_glm2_DSh$coefficients,
    glm2_Sh_result = opt_glm2_Sh$coefficients
  ))
}

test_model <- function(glm_coefficients, data_X, data_Y) {
  
  upper_limit <- 3  # =3 for 4 classes; =9 for 10 classes
  
  ### Model 1 ---> OLS ###
  predicted_glm2_OLS <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_OLS_result)))
  predicted_glm2_OLS <- ifelse(predicted_glm2_OLS <= upper_limit, predicted_glm2_OLS, upper_limit)
  
  ### Model 2 ---> SR ###
  predicted_glm2_SR <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_SR_result)))
  predicted_glm2_SR <- ifelse(predicted_glm2_SR <= upper_limit, predicted_glm2_SR, upper_limit)
  
  ### Model 3 ---> GSR ###
  predicted_glm2_GSR <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_GSR_result)))
  predicted_glm2_GSR <- ifelse(predicted_glm2_GSR <= upper_limit, predicted_glm2_GSR, upper_limit)
  
  ### Model 4 ---> St ###
  predicted_glm2_St <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_St_result)))
  predicted_glm2_St <- ifelse(predicted_glm2_St <= upper_limit, predicted_glm2_St, upper_limit)
  
  ### Model 5 ---> DSh ###
  predicted_glm2_DSh <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_DSh_result)))
  predicted_glm2_DSh <- ifelse(predicted_glm2_DSh <= upper_limit, predicted_glm2_DSh, upper_limit)
  
  ### Model 6 ---> Sh ###
  predicted_glm2_Sh <- floor(exp(data_X %*% as.matrix(glm_coefficients$glm2_Sh_result)))
  predicted_glm2_Sh <- ifelse(predicted_glm2_Sh <= upper_limit, predicted_glm2_Sh, upper_limit)
  
  print(max(predicted_glm2_OLS))
  print(max(predicted_glm2_SR))
  
  r_OLS <- c(mse(data_Y, predicted_glm2_OLS), rmse(data_Y, predicted_glm2_OLS), mae(data_Y, predicted_glm2_OLS))
  r_SR <- c(mse(data_Y, predicted_glm2_SR), rmse(data_Y, predicted_glm2_SR), mae(data_Y, predicted_glm2_SR))
  r_GSR <- c(mse(data_Y, predicted_glm2_GSR), rmse(data_Y, predicted_glm2_GSR), mae(data_Y, predicted_glm2_GSR))
  r_St <- c(mse(data_Y, predicted_glm2_St), rmse(data_Y, predicted_glm2_St), mae(data_Y, predicted_glm2_St))
  r_DSh <- c(mse(data_Y, predicted_glm2_DSh), rmse(data_Y, predicted_glm2_DSh), mae(data_Y, predicted_glm2_DSh))
  r_Sh <- c(mse(data_Y, predicted_glm2_Sh), rmse(data_Y, predicted_glm2_Sh), mae(data_Y, predicted_glm2_Sh))
  
  return(list(
    results_OLS = r_OLS,
    results_SR = r_SR,
    results_GSR = r_GSR,
    results_St = r_St,
    results_DSh = r_DSh,
    results_Sh = r_Sh
  ))
}
```

```{r, eval=FALSE}
out_of_sample_performance <- function(percentage = 0.3, no_trials = 50, datain = cybersickness_10lags, fileout = "results10.csv") {
  
  data <- datain
  agregated_results<-rep(0,4*3)
  data<-set_classes(data)   
  data<-standardize_features(data)
  
  for (r in 1:no_trials){
    bound <- floor(nrow(data)*(1-percentage)) 
    data <- data[sample(nrow(data)), ]      
    train <- data[1:bound, ]            
    test <- data[(bound+1):nrow(data), ]
    
    X.tilde <- train[,-ncol(train)]
    X <-  X.tilde[,-1]
    train_X <- as.matrix(train[,-ncol(train)])
    train_Y <-  train[,ncol(train)]
    glm_coefficients<-fit_and_return_coefficients(train_X, train_Y)
    
    test_X <- as.matrix(test[,-ncol(test)])
    test_Y <-  as.matrix(test[,ncol(test)])
    results<-test_model(glm_coefficients, test_X, test_Y)
    agregated_results<-agregated_results+unlist(results)
  }
  
  df <- data.frame(matrix(unlist(agregated_results), nrow=length(results), byrow=TRUE))
  colnames(df) <- c("MSE", "RMSE", "MAE")
  rownames(df) <- c("GLM2", "SR", "GSR", "St", "DSh", "Sh")
  write.csv(df, fileout)
}

datain <- cybersickness_10lags
output_file_path <- "results10.csv"

run_performance_test <- out_of_sample_performance(
  percentage = 0.3,         
  no_trials = 50,            
  datain = datain,  
  fileout = output_file_path 
)
```

## Portfolio Investment

This second example applies shrinkage methods to asset return data for constructing optimal portfolios. We use the `returns_441` dataset, included in the package under `data/returns_441.rda`. The dataset consists of daily returns for 441 stocks including the index, with `Date` formatted as `YYYYMMDD`. For each rolling window, we:

- Fit multiple regression-based models (OLS, RR, St, DSh, Sh, SR, GSR, SRR)
- Extract the estimated coefficients and compute portfolio weights
- Simulate out-of-sample portfolio performance
- Compute rolling statistics: expected annual returns, volatility, and Sharpe ratios
```{r, eval=FALSE}
library(savvySh)
library(MASS)
library(glmnet)
library(PerformanceAnalytics)
library(lubridate)
library(quadprog)
library(xts)

data <- returns_441
data$Date <- as.Date(as.character(data$Date), format = "%Y%m%d")
colnames(data)[2:442] <- paste0("Company", 1:441)

training_size <- 5 * 252  
testing_size  <- 3 * 21  
step_size     <- 3 * 21   
n_total <- nrow(data)
max_windows <- floor((n_total - training_size - testing_size) / step_size) + 1
cat("Total rows:", n_total, "\n")
cat("Max windows:", max_windows, "\n")

get_full_weights <- function(est_vector) {
  w <- est_vector[-1]      
  w_last <- 1 - sum(w)    
  return(c(w, w_last))
}
```

```{r, eval=FALSE}
rolling_annual_expected_returns <- data.frame()
rolling_annual_sharpe_ratios <- data.frame()
rolling_annual_volatilities <- data.frame()
for (window_index in seq_len(max_windows)) {
  start_index <- 1 + (window_index - 1) * step_size
  train_start <- start_index
  train_end   <- start_index + training_size - 1
  test_start  <- train_end + 1
  test_end    <- train_end + testing_size
  
  train_data <- data[train_start:train_end, ]
  test_data  <- data[test_start:test_end, ]
  train_returns <- as.matrix(train_data[, -1])  
  test_returns  <- as.matrix(test_data[, -1]) 
  Y_train <- train_returns[, 441]
  X_train <- matrix(Y_train, nrow = nrow(train_returns), ncol = 440) - train_returns[, 1:440]
  
  beta_ols <- coef(lm(Y_train ~ X_train))
  glmnet_fit <- cv.glmnet(X_train, Y_train, alpha = 0)
  lambda_min_RR_glmnet <- glmnet_fit$lambda.min
  beta_RR <- as.vector(coef(glmnet_fit, s = "lambda.min"))
  multi_results <- savvySh(X_train, Y_train, model_class = "Multiplicative", include_Sh = TRUE)
  slab_results <- savvySh(X_train, Y_train, model_class = "Slab")
  srr_results <- savvySh(X_train, Y_train, model_class = "ShrinkageRR")
  
  est_results <- list(
    OLS  = beta_ols,
    RR   = beta_RR,
    St   = coef(multi_results, "St"),
    DSh  = coef(multi_results, "DSh"),
    Sh   = coef(multi_results, "Sh"), 
    SR   = coef(slab_results, "SR"),
    GSR  = coef(slab_results, "GSR"),
    SRR  = coef(srr_results, "SRR")
  )
  weights_list <- lapply(est_results, get_full_weights)
  names(weights_list) <- names(est_results)
  
  test_dates <- as.Date(test_data$Date)
  test_returns_xts <- xts(test_returns, order.by = test_dates)
  daily_returns_list <- lapply(weights_list, function(w) {
    rp <- Return.portfolio(R = test_returns_xts, weights = w)
    return(as.numeric(rp))
  })
  
  daily_values_list <- lapply(daily_returns_list, function(r) {
    cum_val <- cumprod(1 + r)
    return(cum_val)
  })
  
  model_names <- names(daily_returns_list)
  n_test <- length(test_start:test_end)
  daily_values_mat <- matrix(0, nrow = length(model_names), ncol = n_test)
  daily_returns_mat <- matrix(0, nrow = length(model_names), ncol = n_test)
  rownames(daily_values_mat) <- model_names
  rownames(daily_returns_mat) <- model_names
  
  for (i in seq_along(model_names)) {
    daily_values_mat[i, ]  <- daily_values_list[[i]]
    daily_returns_mat[i, ] <- daily_returns_list[[i]]
  }
  
  expected_daily_returns <- rowMeans(daily_returns_mat)
  annual_returns <- (1 + expected_daily_returns)^252 - 1
  daily_vols <- apply(daily_returns_mat, 1, sd)
  annual_vols <- daily_vols * sqrt(252)
  annual_sharp <- annual_returns / annual_vols

  window_result_returns <- as.data.frame(t(annual_returns))
  window_result_returns$Window <- window_index
  rolling_annual_expected_returns <- rbind(rolling_annual_expected_returns, window_result_returns)
  
  window_result_sharpe <- as.data.frame(t(annual_sharp))
  window_result_sharpe$Window <- window_index
  rolling_annual_sharpe_ratios <- rbind(rolling_annual_sharpe_ratios, window_result_sharpe)
  
  window_result_vols <- as.data.frame(t(annual_vols))
  window_result_vols$Window <- window_index
  rolling_annual_volatilities <- rbind(rolling_annual_volatilities, window_result_vols)
  
  window_label <- paste0("window_", window_index)
  write.csv(daily_values_mat, 
            file = paste0("daily_portfolio_values_", window_label, ".csv"), 
            row.names = TRUE)
  write.csv(daily_returns_mat, 
            file = paste0("daily_portfolio_returns_", window_label, ".csv"), 
            row.names = TRUE)
  
  cat("Completed window", window_index,
      ": Training rows [", train_start, "to", train_end, 
      "] (Dates:", format(train_data$Date[1], "%Y-%m-%d"), 
      "to", format(train_data$Date[nrow(train_data)], "%Y-%m-%d"), 
      "), Testing rows [", test_start, "to", test_end, 
      "] (Dates:", format(test_data$Date[1], "%Y-%m-%d"), 
      "to", format(test_data$Date[nrow(test_data)], "%Y-%m-%d"), ")\n")
}
```

---

